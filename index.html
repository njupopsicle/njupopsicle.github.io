<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="nju.css" type="text/css" />
<title>Ying Tai's Homepage</title>
</head>
<body>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="image-container">
    <img src="./projects/1200px-NJU.svg.png" width="90px" height="112px" alt="NJU">
</div>
<div class="menu-item"><a href="index.html" class="current">Homepage</a></div>
<div class="menu-item"><a href="pub.html">Publications</a></div>
<div class="menu-item"><a href="group.html">Members</a></div>
<div class="menu-item"><a href="service.html">Services</a></div>
<div class="menu-item"><a href="award.html">Awards</a></div>
<!-- <div class="menu-item"><a href="join.html">Join&nbsp;us</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Ying Tai's Homepage</h1>
</div>
<table class="imgtable"><tr><td>
<img src="./projects/yingtai.JPG" alt="Ying Tai's profile picture" width="400px" height="280px" />&nbsp;</td>
<td align="left"><p><b>Dr. Ying Tai</b>
</p>
<p><b>Associate Professor</b>
</p>
<p><a href="https://njusz.nju.edu.cn/main.htm" target=&ldquo;blank&rdquo;>Nanjing University (Suzhou Campus)</a>
</p>
<p>1520 Taihu Road, Suzhou, P.R. China
</p>
<p><b>Email:</b> tyshiwo(at)gmail.com
</p>
<p><a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a> | 
  <a href="https://github.com/tyshiwo/" target=&ldquo;blank&rdquo;>Github</a> ｜ 
  <a href="https://www.scopus.com/authid/detail.uri?authorId=56437886200">Scopus</a>
</p>
</td></tr></table>

<div class="infoblock">
<div class="blocktitle"></div>
<div class="blockcontent">
<p><b><font color=red>Looking for self-motivated graduate students (both Ph.D. and master) working with me. For prospective students, please send me your resume and transcript. 
</font></b></p>
<p><b><font color=red><span lang="zh-CN" xml:lang="zh-CN"> Opening positions: 2024年秋季入学硕士、博士名额若干个，欢迎感兴趣的同学邮件联系我，正常我会在收到邮件后的1-2周内回复，谢谢！</span>
</font></b></p>
</div></div>
<h2>Biography</h2>
<p>I will be joining Nanjing University (Suzhou Campus) as an Associate Professor in Aug. 2023. 
  Previously, I was a Principal Researcher and Team Lead at Tencent Youtu Lab, where I spent more than 6 wonderful years, leading two teams developing novel vision algorithms that are applied in several products, e.g., Virtual Background feature in Tencent Meeting, High-fidelity face generation APIs in Tencent Cloud and Talking Face Generation for digital human product. Also, our team conducted cutting-edge research works that are published in top-tier AI conferences.
</p>
<p>I got my Ph.D. degree from the Department of Computer Science and Engineering, Nanjing University of Science & Technology (NUST) in 2017, and my advisor is Prof. <a href="http://www.patternrecognition.cn/~jian/">Jian Yang</a>. In 2016, I spent 6 wonderful months as a visiting student at Prof. <a href="http://www.cse.msu.edu/~liuxm/">Xiaoming Liu</a>'s lab in Michigan State University. 
</p>
<p>My research interests include Frontier Generative AI research and applications based on advanced large vision and language models. Specifically, I work on 
</p>
<ul>
<li><p>Human-centric text-to-image editing and generation
</p>
</li>
<li><p>High-fidelity image/video restoration 
</p>
</li>
<li><p>Multi-modal understanding (vision and language) & generation
</p>
</li>
<li><p>virtual digital human (2D and 3D)
</p>
</li>
</ul>
<h2>News</h2>
<ul>
  <li><p style="text-align:left">07/2023 – I will be an Associate Editor for <a href="https://www.sciencedirect.com/journal/image-and-vision-computing"; style="color: #EE7F2D;"> <b>Image and Vision Computing</b></a> </p></li>
  <li><p style="text-align:left">07/2023 – 1 paper accepted by ICCV'23 </p></li>
  <li><p style="text-align:left">05/2023 – I will be an Area Chair for <a href="https://wacv2024.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2024</b></a> </p></li>
  <li><p style="text-align:left">03/2023 – 3 papers accepted by CVPR'23 </p></li>
  <li><p style="text-align:left">11/2022 – 2 papers accepted by AAAI'23 (1 Oral and 1 Poster) </p></li>
  <li><p style="text-align:left">09/2022 – 1 paper accepted by ACM Transactions on Graphics 2022 </p></li>
  <li><p style="text-align:left">07/2022 – 5 papers accepted by ECCV'22 </p></li>
  <li><p style="text-align:left">06/2022 – Our CDSR on blind super resolution is accepted by ACM MM'22, with the acceptance rate to be <b>27.9%</b> </p></li>
  <li><p style="text-align:left">06/2022 – Our AutoGAN-Synthesizer on MRI reconstruction is accepted by MICCAI'22 </p></li>
  <li><p style="text-align:left">05/2022 – I will be Area Chairs for <a href="https://wacv2023.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2023</b></a> and <a href="http://fg2023.ieee-biometrics.org/organizers/"; style="color: #EE7F2D;"> <b>FG 2023</b></a> </p></li>
  <li><p style="text-align:left">04/2022 – Our HifiHead on high-fidelity Neural Head Synthesis is accepted by IJCAI'22, with the acceptance rate to be <b>15%</b> </p></li>
  <li><p style="text-align:left">03/2022 – Our face recognition work <a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;"> <b>CurricularFace (CVPR'20)</b></a> is inlcuded in <a href="https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf"; style="color: #EE7F2D;"> <b>2022 AI index report from Stanford University</b></a> </p></li>
  <li><p style="text-align:left">03/2022 – 5 papers accepted by CVPR'22, with the acceptance rate to be <b>25.3%</b> </p></li>
  <li><p style="text-align:left">03/2022 – First Prize of Progress in Science and Technology of Jiangsu Province (4/11), <a href="http://www.jiangsu.gov.cn/module/download/downfile.jsp?classid=0&filename=d926727e803e49b788d4d7e909a5949e.pdf"; style="color: #EE7F2D;"> <b>“Image restoration and robust recognition: theory and algorithms”</b></a></p></li>
  <li><p style="text-align:left">02/2022 – I will be an Area Chair for <a href="https://eccv2022.ecva.net/organizers/"; style="color: #EE7F2D;"> <b>ECCV 2022</b></a></p></li>
  <li><p style="text-align:left">12/2021 – 3 papers accepted by AAAI'22 (1 Oral and 2 Posters), with the acceptance rate to be <b>15%</b> </p></li>
  <li><p style="text-align:left">09/2021 – 2 papers on blind SR and ViT accepted by NeurIPS'21, with the acceptance rate to be <b>26%</b> </p></li>
  <li><p style="text-align:left">07/2021 – 2 papers on crowd counting accepted by ICCV'21 (1 Oral and 1 Poster), with the acceptance rate to be <b>25.9%</b> </p></li>
  <li><p style="text-align:left">07/2021 – Our <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;"> <b>ASFD</b></a> on face detection is accepted by ACM MM'21 </p></li>
  <li><p style="text-align:left">04/2021 – 4 papers accepted by IJCAI'21, with the acceptance rate to be <b>13.9%</b> </p></li>
  <li><p style="text-align:left">04/2021 – Our Team <b>Imagination</b> is the winner of CVPR NTIRE 2021 Challenge on Video Spatial-Temporal Super-Resolution</p></li>
  <li><p style="text-align:left">03/2021 – 3 papers accepted by CVPR'21 (1 Oral and 2 Posters), with the acceptance rate to be <b>23.7%</b> </p></li>
  <li><p style="text-align:left">12/2020 – 4 papers accepted by AAAI'21, with the acceptance rate to be <b>21%</b> </p></li>
  <li><p style="text-align:left">09/2020 – Training codes of RealSR are available in Tencent official github account <a href="https://github.com/Tencent/Real-SR" target="_blank" rel="external">[Tencent-RealSR]</a>. </p></li>
  <li><p style="text-align:left">07/2020 – 6 papers accepted by ECCV'20, with the acceptance rate to be <b>27%</b> </p></li>
  <li><p style="text-align:left">05/2020 – Our RealSR model (Team name: Impressionism) won both tracks of <a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a> </p></li>
  <li><p style="text-align:left">02/2020 – 3 papers accepted by CVPR'20, with the acceptance rate to be <b>22.1%</b> </p></li>
  <li><p style="text-align:left">11/2019 – 2 papers (Action Proposal & Action Recognition) accepted by AAAI'20, with the acceptance rate to be <b>20.6%</b>. The code of our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> is released at <a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG" target="_blank" rel="external">[ActionDetection-DBG]</a>, which achieves <b>Top 1</b> performance on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a> </p></li>
  <li><p style="text-align:left">02/2019 – Our <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>DSFD</b></a> on face detection is accepted by CVPR'19, with the acceptance rate to be <b>25.2%</b> </p></li>
  <li><p style="text-align:left">11/2018 – 2 papers (face alignement & adaptive metric learning) accepted by AAAI'19, with the acceptance rate to be ONLY <b>16.2%</b> </p></li>
  <li><p style="text-align:left">10/2018 – We released a novel <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>Dual Shot Face Detector (DSFD) </b> </a> framework that achieves <b>Top 1</b> performance on all FIVE settings of <b>WIDER FACE (Easy/Medium/Hard)</b> and <b>FDDB (Discrete/Continuous)</b> datasets </p></li>
  <li><p style="text-align:left">07/2018 – 1 paper accepted by ECCV'18 </p></li>
  <li><p style="text-align:left">02/2018 – 1 paper accepted by CVPR'18 （SPOTLIGHT Presentation）</p></li>  
  <li><p style="text-align:left">07/2017 – 1 paper accepted by ICCV'17 (SPOTLIGHT Presentation) </p></li>
  <li><p style="text-align:left">03/2017 – 1 paper accepted by CVPR'17 </p></li>
</ul>

<br>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=faf9f9&w=300&t=tt&d=hvoHWGKZcyFl6zhd6aLhpusD9f4jQY_gzPG8UfsmW0I&co=1285d6'></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<div id="footer">
<div id="footer-text">
Last modified in July 2023, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
  
</td>
</tr>
</table>
</body>
</html>
