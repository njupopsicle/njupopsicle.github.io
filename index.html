<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ying Tai(邰颖)</title>
  
  <meta name="author" content="Ying Tai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><strong><font size="4px">Ying Tai(邰颖)</font></strong></name>
              </p>
              <p>I am a Principal Researcher and Team Lead at Tencent Youtu Lab, where I work on computer vision and machine learning.
              </p>
              <p>
              I got my Ph.D. degree from the Department of Computer Science and Engineering, Nanjing University of Science & Technology (NUST) in 2017, and my advisor is Prof. <a href="http://www.patternrecognition.cn/~jian/">Jian Yang</a>. In 2016, I spent 6 wonderful months as a visiting student at Prof. <a href="http://www.cse.msu.edu/~liuxm/">Xiaoming Liu</a>'s lab in Michigan State University. 
              <p>
              Most of my works have released codes in <a href="https://github.com/TencentYoutuResearch" target="_blank" rel="external">[TencentYoutuResearch]</a>, with over <b>4.0K stars</b> and <b>750 forks</b>.
              </p>
              <p style="text-align:center">
                <a href="mailto:tyshiwo@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/tyshiwo/">Github</a> &nbsp/&nbsp
		<a href="https://www.scopus.com/authid/detail.uri?authorId=56437886200">Scopus</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ying-tai-33a76ab6/">LinkedIn</a> 
              </p>
            </td>
            
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./projects/dog&me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./projects/dog&me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
<p style="text-align:left"><strong><font size="4px">Recent news</font></strong></p> 
		<ul>
		  <li><p style="text-align:left">05/2023 – I will be an Area Chair for <a href="https://wacv2024.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2024</b></a> </p></li>
		  <li><p style="text-align:left">03/2023 – 3 papers accepted by CVPR'23 </p></li>
		  <li><p style="text-align:left">11/2022 – 2 papers accepted by AAAI'23 (1 Oral and 1 Poster) </p></li>
		  <li><p style="text-align:left">09/2022 – 1 paper accepted by ACM Transactions on Graphics 2022 </p></li>
		  <li><p style="text-align:left">07/2022 – 5 papers accepted by ECCV'22 </p></li>
		  <li><p style="text-align:left">06/2022 – Our CDSR on blind super resolution is accepted by ACM MM'22, with the acceptance rate to be <b>27.9%</b> </p></li>
		  <li><p style="text-align:left">06/2022 – Our AutoGAN-Synthesizer on MRI reconstruction is accepted by MICCAI'22 </p></li>
		  <li><p style="text-align:left">05/2022 – I will be Area Chairs for <a href="https://wacv2023.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2023</b></a> and <a href="http://fg2023.ieee-biometrics.org/organizers/"; style="color: #EE7F2D;"> <b>FG 2023</b></a> </p></li>
		  <li><p style="text-align:left">04/2022 – Our HifiHead on high-fidelity Neural Head Synthesis is accepted by IJCAI'22, with the acceptance rate to be <b>15%</b> </p></li>
		  <li><p style="text-align:left">03/2022 – Our face recognition work <a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;"> <b>CurricularFace (CVPR'20)</b></a> is inlcuded in <a href="https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf"; style="color: #EE7F2D;"> <b>2022 AI index report from Stanford University</b></a> </p></li>
		  <li><p style="text-align:left">03/2022 – 5 papers accepted by CVPR'22, with the acceptance rate to be <b>25.3%</b> </p></li>
		  <li><p style="text-align:left">03/2022 – First Prize of Progress in Science and Technology of Jiangsu Province (4/11), <a href="http://www.jiangsu.gov.cn/module/download/downfile.jsp?classid=0&filename=d926727e803e49b788d4d7e909a5949e.pdf"; style="color: #EE7F2D;"> <b>“Image restoration and robust recognition: theory and algorithms”</b></a></p></li>
		  <li><p style="text-align:left">02/2022 – I will be an Area Chair for <a href="https://eccv2022.ecva.net/organizers/"; style="color: #EE7F2D;"> <b>ECCV 2022</b></a></p></li>
		  <li><p style="text-align:left">12/2021 – 3 papers accepted by AAAI'22 (1 Oral and 2 Posters), with the acceptance rate to be <b>15%</b> </p></li>
		  <li><p style="text-align:left">09/2021 – 2 papers on blind SR and ViT accepted by NeurIPS'21, with the acceptance rate to be <b>26%</b> </p></li>
		  <li><p style="text-align:left">07/2021 – 2 papers on crowd counting accepted by ICCV'21 (1 Oral and 1 Poster), with the acceptance rate to be <b>25.9%</b> </p></li>
		  <li><p style="text-align:left">07/2021 – Our <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;"> <b>ASFD</b></a> on face detection is accepted by ACM MM'21 </p></li>
		  <li><p style="text-align:left">04/2021 – 4 papers accepted by IJCAI'21, with the acceptance rate to be <b>13.9%</b> </p></li>
		  <li><p style="text-align:left">04/2021 – Our Team <b>Imagination</b> is the winner of CVPR NTIRE 2021 Challenge on Video Spatial-Temporal Super-Resolution</p></li>
		  <li><p style="text-align:left">03/2021 – 3 papers accepted by CVPR'21 (1 Oral and 2 Posters), with the acceptance rate to be <b>23.7%</b> </p></li>
		  <li><p style="text-align:left">12/2020 – 4 papers accepted by AAAI'21, with the acceptance rate to be <b>21%</b> </p></li>
		  <li><p style="text-align:left">09/2020 – Training codes of RealSR are available in Tencent official github account <a href="https://github.com/Tencent/Real-SR" target="_blank" rel="external">[Tencent-RealSR]</a>. </p></li>
		  <li><p style="text-align:left">07/2020 – 6 papers accepted by ECCV'20, with the acceptance rate to be <b>27%</b> </p></li>
		  <li><p style="text-align:left">05/2020 – Our RealSR model (Team name: Impressionism) won both tracks of <a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a> </p></li>
		  <li><p style="text-align:left">02/2020 – 3 papers accepted by CVPR'20, with the acceptance rate to be <b>22.1%</b> </p></li>
	          <li><p style="text-align:left">11/2019 – 2 papers (Action Proposal & Action Recognition) accepted by AAAI'20, with the acceptance rate to be <b>20.6%</b>. The code of our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> is released at <a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG" target="_blank" rel="external">[ActionDetection-DBG]</a>, which achieves <b>Top 1</b> performance on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a> </p></li>
	          <li><p style="text-align:left">02/2019 – Our <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>DSFD</b></a> on face detection is accepted by CVPR'19, with the acceptance rate to be <b>25.2%</b> </p></li>
                  <li><p style="text-align:left">11/2018 – 2 papers (face alignement & adaptive metric learning) accepted by AAAI'19, with the acceptance rate to be ONLY <b>16.2%</b> </p></li>
                  <li><p style="text-align:left">10/2018 – We released a novel <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>Dual Shot Face Detector (DSFD) </b> </a> framework that achieves <b>Top 1</b> performance on all FIVE settings of <b>WIDER FACE (Easy/Medium/Hard)</b> and <b>FDDB (Discrete/Continuous)</b> datasets </p></li>
	          <li><p style="text-align:left">07/2018 – 1 paper accepted by ECCV'18 </p></li>
	          <li><p style="text-align:left">02/2018 – 1 paper accepted by CVPR'18 （SPOTLIGHT Presentation）</p></li>	
	          <li><p style="text-align:left">07/2017 – 1 paper accepted by ICCV'17 (SPOTLIGHT Presentation) </p></li>
	          <li><p style="text-align:left">03/2017 – 1 paper accepted by CVPR'17 </p></li>
	        </ul>
	 
	<br>
	<p style="text-align:justify"><strong><font size="4px">Preprints</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
			

            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ImAR_arXIv2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.08612.pdf">
              <papertitle><b>Learning Versatile 3D Shape Generation with Improved AR Models</b></papertitle>
              </a>
              <br>
              S. Luo, X. Qian, Y. Fu, Y. Zhang, <b>Y. Tai</b>, Z. Zhang, C. Wang, and X. Xue.              
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.14700.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr> 
			
	   <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CtlGAN_arXiv2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.08612.pdf">
              <papertitle><b>CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning</b></papertitle>
              </a>
              <br>
              Y. Wang, R. Yi, <b>Y. Tai</b>, C. Wang, and L. Ma.              
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.08612.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr> 
			
	   <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/STC_arXiv2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2202.03747.pdf">
              <papertitle><b>STC: Spatio-Temporal Contrastive Learning for Video Instance Segmentation</b></papertitle>
              </a>
              <br>
              Z. Jiang*, Z. Gu*, J. Peng, H. Zhou, L. Liu, Y. Wang, <b>Y. Tai</b>, C. Wang, L. Zhang.              
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2202.03747.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CFNet-arXiv22.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2201.04796.pdf">
              <papertitle><b>CFNet: Learning Correlation Functions for One-Stage Panoptic Segmentation</b></papertitle>
              </a>
              <br>
              Y. Chen*, W. Chu*, F. Wang, <b>Y. Tai</b>, R. Yi, Z. Gan, L. Yao, C. Wang and X. Li.              
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2201.04796.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr>
			
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/Arxiv20-Style.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf">
              <papertitle><b>Collaborative Learning for Faster StyleGAN Embedding</b></papertitle>
              </a>
              <br>
              S. Guan, <b>Y. Tai</b>, B. Ni, F. Zhu, F. Huang and X. Yang.              
              <br>
              <em>arXiv</em>, 2020 
              <br>
              <a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr>


            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AG_arxiv19.JPG" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.00713">
              <papertitle><b>Aurora Guard: Reliable Face Anti-Spoofing via Mobile Lighting System</b></papertitle>
              </a>
              <br>
              J. Zhang, <b>Y. Tai</b>, T. Yao, J. Meng, S. Ding, C. Wang, J. Li, F. Huang and R. Ji.             
              <br>
              <em>arXiv</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2102.00713"; style="color: #EE7F2D;">arXiv</a>
            
            </td><tr>
			
        </table></p>
 
 <p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/NPF-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link">
              <papertitle><b>Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild</b></papertitle>
              </a>
              <br>
	      Z. Zhang, R. Chen, W. Cao, <b>Y. Tai</b><sup>#</sup>, and C. Wang<sup>#</sup>
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
	      <a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	    
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CALoss-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link">
              <papertitle><b>Learning to Measure the Point Cloud Reconstruction Loss in a Representation Space</b></papertitle>
              </a>
              <br>
	      T. Huang, Z. Ding, J. Zhang, <b>Y. Tai</b>, Z. Zhang, M. Chen, C. Wang, and Y. Liu
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
	      <a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HiTalk-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.02572.pdf">
              <papertitle><b>High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning</b></papertitle>
              </a>
              <br>
	      C. Xu, J. Zhang, J. Zhu, W. Chu, <b>Y. Tai</b>, C. Wang, and Y. Liu
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
	      <a href="https://arxiv.org/pdf/2305.02572.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HitNet_AAAI2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.11624.pdf">
              <papertitle><b>High-resolution Iterative Feedback Network for Camouflaged Object Detection</b></papertitle>
              </a>
              <br>
	      X. Hu, S. Wang, X. Qian, H. Dai, W. Ren, D. Luo, <b>Y. Tai</b>, and L. Shao
              <br>
              <em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023
              <br>
	      <a href="https://arxiv.org/pdf/2203.11624.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>
          
	
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CRI_AAAI2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2302.03406.pdf">
              <papertitle><b>High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets</b></papertitle>
              </a>
              <br>
	      Y. Wang, C. Lin, D. Luo, <b>Y. Tai</b>, Z. Zhang and Y. Xie
              <br>
              <em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023 <b style="color: red">[Oral]</b> 
              <br>
	      <a href="https://arxiv.org/pdf/2302.03406.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>		

	
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/3QNet_TOG2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing">
              <papertitle><b>3QNet: 3D Point Cloud Geometry Quantization Compression Network</b></papertitle>
              </a>
              <br>
	      T. Huang, J. Zhang, J. Chen, Z. Ding, <b>Y. Tai</b>, Z. Zhang, C. Wang, and Y. Liu
              <br>
              <em>ACM Transactions on Graphics</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ColorFormer_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing">
              <papertitle><b>ColorFormer: Image Colorization via Color Memory assisted Hybrid-attention Transformer</b></papertitle>
              </a>
              <br>
	      X. Ji*, B. Jiang*, D. Luo, G. Tao, W. Chu, <b>Y. Tai</b><sup>#</sup>, Z. Xie, and C. Wang<sup>#</sup>
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		    /
	      <a href="https://drive.google.com/file/d/1-hhTHW9aQ60JJqYKIs-6tEB1xtesjG60/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/StyleFace_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing">
              <papertitle><b>StyleFace: Towards Identity-Disentangled Face Generation on Megapixels</b></papertitle>
              </a>
              <br>
	      Y. Luo, J. Yan, J. Zhu, K. He, W. Chu,  <b>Y. Tai</b>, and C. Wang
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		   /
	      <a href="https://drive.google.com/file/d/1ncWoQXQRa7Sg_dKSpbycX0x2ieHlHZJB/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>  
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/FRS_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="google.com">
              <papertitle><b>Designing One Unified Framework for High-Fidelity Face Reenactment and Swapping</b></papertitle>
              </a>
              <br>
	      C. Xu*, J.  Zhang*, Y. Han, G. Tian, X. Zeng, <b>Y. Tai</b>, Y. Wang, C. Wang, and Y. Liu
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1uahR40VteHhBdjRVEN_hPwN9g5r54GZ1/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		    /
	      <a href="https://drive.google.com/file/d/1CDDtFN6txDdALMkq0Qpc2lFGstmtnntv/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
		    /
	      <a href="https://github.com/xc-csc101/UniFace"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/xc-csc101/UniFace.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SeedFormer_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.10315.pdf">
              <papertitle><b>SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer</b></papertitle>
              </a>
              <br>
	      H. Zhou, Y. Cao, W. Chu, J. Zhu, L. Tong, <b>Y. Tai</b>, and C. Wang
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/pdf/2207.10315.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
		    /
	      <a href="https://drive.google.com/file/d/1p2ubWO25o9e0d0hez6hmzXahww5j_dQc/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
		    /
	      <a href="https://github.com/hrzhou2/seedformer"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/hrzhou2/seedformer.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ProCA_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.06654.pdf">
              <papertitle><b>Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation</b></papertitle>
              </a>
              <br>
	      Z. Jiang, Y. Li, C. Yang, P. Gao, Y. Wang, <b>Y. Tai</b>, and C. Wang
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/pdf/2207.06654.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
		    /
	      <a href="https://github.com/jiangzhengkai/ProCA"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jiangzhengkai/ProCA.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CDSR_MM22.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2208.13436.pdf">
              <papertitle><b>Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution</b></papertitle>
              </a>
              <br>
	      Y. Zhou*, C. Lin*, D. Luo, Y. Liu, Mingang Chen, <b>Y. Tai</b><sup>#</sup>, and C. Wang<sup>#</sup>
              <br>
              <em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/pdf/2208.13436.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>
	
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AutoGAN-Synthesizer_MICCAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38">
              <papertitle><b>AutoGAN-Synthesizer: Neural Architecture Searchfor Cross-Modality MRI Synthesis</b></papertitle>
              </a>
              <br>
	      X. Hu, R. Shen, D. Luo, <b>Y. Tai</b>, C. Wang, and B. Menze
              <br>
              <em>International Conference on Medical Image Computing and Computer Assisted Intervention (<b>MICCAI</b>)</em>, 2022
              <br>
	      <a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38"; style="color: #EE7F2D;">Paper (official link)</a>
	      /
	      <a href="https://github.com/HUuxiaobin/AutoGAN-Synthesizer"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HUuxiaobin/AutoGAN-Synthesizer.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HifiHead_IJCAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing">
              <papertitle><b>HifiHead: One-Shot High Fidelity Neural Head Synthesis with 3D Control</b></papertitle>
              </a>
              <br>
              F. Zhu, J. Zhu, W. Chu, <b>Y. Tai</b><sup>#</sup>, Z. Xie, X. Huang, and C. Wang<sup>#</sup>.        
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SGPN_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing">
              <papertitle><b>Blind Face Restoration via Integrating Face Shape and Generative Priors</b></papertitle>
              </a>
              <br>
              F. Zhu, J. Zhu, W. Chu, X. Zhang, X. Ji, C. Wang<sup>#</sup>, and <b>Y. Tai</b><sup>#</sup>.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	       /
	      <a href="https://drive.google.com/file/d/1BhJa5I95sAzENzo0_He-L8vcsYFlKL9N/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
	       /
	      <a href="https://github.com/TencentYoutuResearch/FaceRestoration-sgpn"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/FaceRestoration-sgpn.svg" alt="GitHub stars" title="" />
            </td><tr>
	
	 <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/IFRNet_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing">
              <papertitle><b>IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</b></papertitle>
              </a>
              <br>
              L. Kong*, B. Jiang*, D. Luo, W. Chu, X. Huang, <b>Y. Tai</b>, C. Wang, and J. Yang.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://drive.google.com/file/d/15K_m9HhGQ4MfrUREr0DxcdtEuCnv_OD3/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>    
	      /
	      <a href="https://github.com/ltkong218/IFRNet"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ltkong218/IFRNet.svg" alt="GitHub stars" title="" />
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/phyDIR_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing">
              <papertitle><b>Physically-guided Disentangled Implicit Rendering for 3D Face Modeling</b></papertitle>
              </a>
              <br>
              Z. Zhang, Y. Ge, <b>Y. Tai</b>, W. Cao, R. Chen, K. Liu, H. Tang, X. Huang, C. Wang, Z. Xie, and D. Huang.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://drive.google.com/file/d/1UohmPBmgoo6-Dh7ES2ugLvy9-X2TVDNi/view?usp=sharing"; style="color: #EE7F2D;">Supp</a> 
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/RDF_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing">
              <papertitle><b>Learning to Restore 3D Face from In-the-Wild Degraded Images</b></papertitle>
              </a>
              <br>
              Z. Zhang, Y. Ge, <b>Y. Tai</b>, X. Huang, C. Wang, H. Tang, D. Huang, and Z. Xie.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
              /
	      <a href="https://drive.google.com/file/d/1ahwfZtPB4YrLWXswsDz2jSPGon2ZCiZa/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/MFH_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing">
              <papertitle><b>Learning to Memorize Feature Hallucination for One-Shot Image Generation</b></papertitle>
              </a>
              <br>
              Y. Xie, Y. Fu, J. Zhu, <b>Y. Tai</b>, Y. Cao, and C. Wang.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/DIRL-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing">
              <papertitle><b>DIRL: Domain-invariant Representation Learning for Generalizable Semantic Segmentation</b></papertitle>
              </a>
              <br>
              Q. Xu, L. Yao, Z. Jiang, G. Jiang, W. Chu, W. Han, W. Zhang, C. Wang, and <b>Y. Tai</b><sup>#</sup>.        
              <br>
              <em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022 <b style="color: red">[Oral]</b> 
              <br>
              <a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SCSNet-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing">
              <papertitle><b>SCSNet: Simultaneously Image Colorization and Super-Resolution</b></papertitle>
              </a>
              <br>
              J. Zhang, C. Xu, Y. Han, J. Li, Y. Wang, <b>Y. Tai</b>, C. Wang, F. Huang, Z. Xie, and Y. Liu.      
              <br>
              <em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/LCTR-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing">
              <papertitle><b>LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization</b></papertitle>
              </a>
              <br>
              Z. Chen, C. Wang, Y. Wang, G. Jiang, Y. Shen, <b>Y. Tai</b>, C. Wang, W. Zhang, and L. Cao.       
              <br>
              <em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/S2K-nips2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2110.12151.pdf">
              <papertitle><b>Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution</b></papertitle>
              </a>
              <br>
              G. Tao, X. Ji, W. Wang, S. Chen, C. Lin, Y. Cao, T. Lu, D. Luo, and <b>Y. Tai</b>.             
              <br>
              <em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2110.12151.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p><font color="red">A novel framework S2K that predicts the kernel from spectrum in frequency domain</p>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/EAT-arXiv2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2105.15089.pdf">
              <papertitle><b>Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model</b></papertitle>
              </a>
              <br>
              J. Zhang, C. Xu, J. Li, W. Chen, Y. Wang, <b>Y. Tai</b>, S. Chen, C. Wang, F. Huang and R. Liu.             
              <br>
              <em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2105.15089.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/BaseArchitecture-EAT"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/BaseArchitecture-EAT.svg" alt="GitHub stars" title="" />
              <p><font color="red">An interesting viewpoint between EA and Transfomer, and propose an EA based Transformer framework for both NLP and CV tasks</p>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/P2PNet-ICCV2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2107.12746.pdf">
              <papertitle><b>Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework</b></papertitle>
              </a>
              <br>
              Q. Song*, C. Wang*, Z. Jiang, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Wu.              
              <br>
              <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
              <br>
              <a href="https://arxiv.org/pdf/2107.12746.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-P2PNet.svg" alt="GitHub stars" title="" />
              <p><font color="red">We propose a novel simple and elegant framework for crowd counting, which directly predicts the crowd location instead of using density map estimation adopted in most previous methods.</font></p>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/UEPNet-ICCV2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2107.12619.pdf">
              <papertitle><b>Uniformity in Heterogeneity: Diving Deep into Count Interval Partition for Crowd Counting</b></papertitle>
              </a>
              <br>
              C. Wang*, Q. Song*, B. Zhang, Y. Wang, <b>Y. Tai</b>, X. Hu, C. Wang, J. Li, J. Ma, and Y. Wu.             
              <br>
              <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 
              <br>
	      <a href="https://arxiv.org/pdf/2107.12619.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-UEPNet.svg" alt="GitHub stars" title="" />
            </td><tr>
	
	
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ASFD_arXiv20.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2003.11228.pdf">
              <papertitle><b>ASFD: Automatic and Scalable Face Detector</b></papertitle>
              </a>
              <br>
              J. Li*, B. Zhang*, Y. Wang, <b>Y. Tai</b>, Z. Zhang, C. Wang, J. Li, X. Huang and Y. Xia.              
              <br>
              <em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a></font></p>
            </td><tr>
			
			
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HifiFace-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2106.09965.pdf">
              <papertitle><b>HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping</b></papertitle>
              </a>
              <br>
              Y. Wang*, X. Chen*, J. Zhu, W. Chu, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, Y. Wu, F. Huang and R. Ji.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2106.09965.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://johann.wang/HifiFace/"; style="color: #EE7F2D;">Project</a>
	      /
	      <a href="https://drive.google.com/file/d/1K9QsPX2Yw7iUtH33aoP-dTlQhnZLQuWC/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>	
	      /
	      <a href="https://drive.google.com/file/d/19i-4tJD7NxrqtmlRUl9l3M-FuMy7F2EG/view?usp=sharing"; style="color: #EE7F2D;">Video (1min)</a>	
              </p>
            </td><tr>
	
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SPL-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2106.07220.pdf">
              <papertitle><b>Context-Aware Image Inpainting with Learned Semantic Priors</b></papertitle>
              </a>
              <br>
              W. Zhang, J. Zhu, <b>Y. Tai</b>, Y. Wang, W. Chu, B. Ni, C. Wang and X. Yang.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2106.07220.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://arxiv.org/pdf/2112.04107.pdf"; style="color: #EE7F2D;">Extended journal version</a>
	      /
	      <a href="https://github.com/WendongZh/SPL"; style="color: #EE7F2D;">Code (Official)</a>    
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/WendongZh/SPL.svg" alt="GitHub stars" title="" />
              </p>
            </td><tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/DRDG-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2106.16128.pdf">
              <papertitle><b>Dual Reweighting Domain Generalization for Face Presentation Attack Detection</b></papertitle>
              </a>
              <br>
              S. Liu, K. Zhang, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, Y. Xie and L. Ma.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2106.16128.pdf"; style="color: #EE7F2D;">arXiv</a>
              </p>
            </td><tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SiamRCR-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2105.11237.pdf">
              <papertitle><b>SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</b></papertitle>
              </a>
              <br>
              J. Peng*, Z. Jiang*, Y. Gu*, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang and W. Lin.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2105.11237.pdf"; style="color: #EE7F2D;">arXiv</a>
              </p>
            </td><tr>
		    
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/3DFace_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf">
              <papertitle><b>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</b></papertitle>
              </a>
              <br>
              Z. Zhang, Y. Ge, R. Chen, <b>Y. Tai</b>, Y. Yan, J. Yang, C. Wang, J. Li, and F. Huang.              
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/3DFaceReconstruction-LAP"; style="color: #EE7F2D;">Code (Official)</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/3DFaceReconstruction-LAP.svg" alt="GitHub stars" title="" />
              </p>
            </td><tr>		
	
		           
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AFSD_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.13137">
              <papertitle><b>Learning Salient Boundary Feature for Anchor-free Temporal Action Localization</b></papertitle>
              </a>
              <br>
              C. Lin*, C. Xu*, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.              
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2103.13137"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/ActionDetection-AFSD"; style="color: #EE7F2D;">Code (Official)</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionDetection-AFSD.svg" alt="GitHub stars" title="" />
              </p>
            </td><tr>	
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/REVIDE_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"><papertitle><b>Learning to Restore Hazy Video: A New Real-World Dataset and A New Method</b></papertitle></a><br>
		X. Zhang*, H. Dong*, J. Pan, C. Zhu, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and F. Wang.
		<br>
                <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
		<br>
		<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
		
		</td></tr>

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/FCA_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2012.10102.pdf"><papertitle><b>Frequency Consistent Adaptation for Real World Super Resolution</b></papertitle></a><br>
		X. Ji*, G. Tao*, Y. Cao, <b>Y. Tai</b>, T. Lu, C. Wang, J. Li, and F. Huang.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021 
		<br>
		<a href="https://arxiv.org/pdf/2012.10102.pdf"; style="color: #EE7F2D;">arXiv</a>
		</p>
		<p><font color="red">Improved version of our prior work RealSR</font></p>
		</td></tr>			  
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CMR_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"><papertitle><b>Learning Comprehensive Motion Representation for Action Recognition</b></papertitle></a><br>
		M. Wu*, B. Jiang*, D. Luo, J. Yan, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, and X. Yang.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"; style="color: #EE7F2D;">arXiv</a>
	        /
	        <a href="https://github.com/TencentYoutuResearch/ActionRecognition-CMR"; style="color: #EE7F2D;">Code (Official)</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionRecognition-CMR.svg" alt="GitHub stars" title="" />
		</p>
		<p><font color="red">Extented version of our prior works TEINet and TDRL</font></p>
		</td></tr>	


          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/D2AM_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2105.02453.pdf"><papertitle><b>Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</b></papertitle></a><br>
		Z. Chen, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, F. Huang, and X. Jin.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://arxiv.org/pdf/2105.02453.pdf"; style="color: #EE7F2D;">arXiv</a>
		</p>
		</td></tr>	

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SASNet_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"><papertitle><b>To Choose or to Fuse? Scale Selection for Crowd Counting</b></papertitle></a><br>
		Q. Song*, C. Wang*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, J. Wu, and J. Ma.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	        /
		<a href="https://github.com/TencentYoutuResearch/CrowdCounting-SASNet"; style="color: #EE7F2D;">Code (Official)</a> 
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-SASNet" alt="GitHub stars" title="" />
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/FAN_ArXiv19.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.11680.pdf"><papertitle><b>FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization</b></papertitle></a><br>
		X. Yin, <b>Y. Tai</b>, Y. Huang and X. Liu.
		<br>
        <em>Asian Conference on Computer Vision (<b>ACCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.11680.pdf"; style="color: #EE7F2D;">Paper</a>
		</p>
		<p><font color="red">Novel framework to improve surveillance face recognition & normalization from unpaired data</font></p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DDL_Arxiv2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2002.03662.pdf"><papertitle><b>Improving Face Recognition from Hard Samples via Distribution Distillation Loss</b></papertitle></a><br>
		Y. Huang*, P. Shen*, <b>Y. Tai</b><sup>#</sup>, S. Li<sup>#</sup>, X. Liu, J. Li, F. Huang, and R. Ji.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/2002.03662.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/Tencent/TFace"; style="color: #EE7F2D;">Code (Official)</a> 
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/TFace.svg" alt="GitHub stars" title="" />
		</p>
		</td></tr>				
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SSCGAN_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"><papertitle><b>SSCGAN: Facial Attribute Editing via Style Skip Connections</b></papertitle></a><br>
		W. Chu, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, F. Huang, and R. Ji.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>		           
            

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2008.08250.pdf"><papertitle><b>Face Anti-Spoofing via Disentangled Representation Learning</b></papertitle></a><br>
		K. Zhang, T. Yao, J. Zhang, <b>Y. Tai</b><sup>#</sup>, S. Ding, J. Li, F. Huang, H. Song and L. Ma.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640630.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CTracker_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2007.14557.pdf"><papertitle><b>Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End <br> Joint Multiple-Object Detection and Tracking</b></papertitle></a><br>
		J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020 <b style="color: red">[Spotlight]</b> 
		<br>
		<a href="https://arxiv.org/pdf/2007.14557.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/pjl1995/CTracker"; style="color: #EE7F2D;">Code (Official)</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/pjl1995/CTracker.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>			

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/TDRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2007.07626.pdf"><papertitle><b>Temporal Distinct Representation Learning for 2D-CNN-based Action Recognition</b></papertitle></a><br>
		J. Weng, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, X. Jiang and J. Yuan.		    <br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520358.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/HPE_arXiv2019.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2008.00697.pdf"><papertitle><b>Adversarial Semantic Data Augmentation for Human Pose Estimation</b></papertitle></a><br>
		Y. Bin, X. Cao, X. Chen, Y. Ge, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, C. Gao and N. Sang.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/2008.00697.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640596.pdf"; style="color: #EE7F2D;">Paper (Official)</a>	
		/
		<a href="https://github.com/Binyr/ASDA"; style="color: #EE7F2D;">Code (Official)</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Binyr/ASDA.svg" alt="GitHub stars" title="" />
		
	    </p>
	    <p><font color="red">State-of-the-art performance on <a href="http://human-pose.mpi-inf.mpg.de/#results"; style="color: #EE7F2D;"> <b>MPII</b></a></font> and <a href="http://sam.johnson.io/research/lsp.html"; style="color: #EE7F2D;"> <b>LSP</b></a></font></p>
		</td></tr>				
		

         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/RealSR_CVPRW2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2005.01996.pdf"><papertitle><b>Real-World Super-Resolution via Kernel Estimation and Noise Injection</b></papertitle></a><br>
		X. Ji, Y. Cao, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition Workshop (<b>CVPRW</b>)</em>, 2020
		<br>
		<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Tencent)</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/Real-SR.svg" alt="GitHub stars" title="" />
		/
		<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Personal)</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jixiaozhong/RealSR.svg" alt="GitHub stars" title="" />
	        /
		<a href="https://github.com/nihui/realsr-ncnn-vulkan"; style="color: #EE7F2D;">Code (NCNN-vulkan)</a>
                <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/nihui/realsr-ncnn-vulkan.svg" alt="GitHub stars" title="" />
		/
		<a href="https://openbenchmarking.org/test/pts/realsr-ncnn"; style="color: #EE7F2D;">OpenBenchmarking.org</a>
		/
		<a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;">Challenge Report</a>
			
	    </p>
	    <p><font color="red">Winner of <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a></font></p>
		</td></tr>				
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CurricularFace_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"><papertitle><b>CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</b></papertitle></a><br>
		Y. Huang, Y. Wang, <b>Y. Tai</b><sup>#</sup>, X. Liu, P. Shen, S. Li<sup>#</sup>, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/HuangYG123/CurricularFace"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HuangYG123/CurricularFace.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>			
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/UnsupervisedOpticalFlow_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"><papertitle><b>Learning by Analogy: Reliable Supervision from Transformations for <br> Unsupervised Optical Flow Estimation</b></papertitle></a><br>
		L. Liu, J. Zhang, Y. Liu, Y. Wang, <b>Y. Tai</b>, D. Luo, C. Wang, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/lliuz/ARFlow"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/lliuz/ARFlow.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>	
	
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/VideoReID_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"><papertitle><b>Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</b></papertitle></a><br>
		Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, <b>Y. Tai</b>, and L. Shao.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/daodaofr/hypergraph_reid"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/daodaofr/hypergraph_reid.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>	
		
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DBG_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.04127.pdf"><papertitle><b>Fast Learning of Temporal Action Proposal via Dense Boundary Generator</b></papertitle></a><br>
		C. Lin*, J. Li*, Y. Wang, <b>Y. Tai</b>, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang and R. Ji.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://github.com/Tencent/ActionDetection-DBG"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/ActionDetection-DBG.svg" alt="GitHub stars" title="" />
			  
	    <p><font color="red">Ranked No. 1 on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a></font></p> 
	    </p>
		</td></tr>					
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/TEI_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.09435.pdf"><papertitle><b>TEINet: Towards an Efficient Architecture for Video Recognition</b></papertitle></a><br>
		Z. Liu*, D. Luo*, Y. Wang, L. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and T. Lu.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.09435.pdf"; style="color: #EE7F2D;">arXiv</a>
	    <br>	    
	    </p>
		</td></tr>	
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/face_detection_arxiv18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://128.84.21.199/abs/1810.10220"><papertitle><b>DSFD: Dual Shot Face Detector</b></papertitle></a><br>
		J. Li, Y. Wang, C. Wang, <b>Y. Tai</b>, J. Qian, J. Yang, C.e Wang, J. Li and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2019
		<br>
		<a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://drive.google.com/open?id=1DuhcD4Thwv8E3kcLZIOQTeo8qaGDq9PB"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/Tencent/FaceDetection-DSFD"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/FaceDetection-DSFD.svg" alt="GitHub stars" title="" />
		
	    <p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a> and <a href="http://vis-www.cs.umass.edu/fddb/results.html#eval"; style="color: #EE7F2D;"> <b>FDDB</b></a> (Until 2019.01)</font></p> 
	    </p>
		</td></tr>
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/aaai19_FHR.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1811.00342.pdf"><papertitle><b>Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos</b></papertitle></a><br>
		<b>Y. Tai*</b>, Y. Liang*, X. Liu, L. Duan, J. Li, C. Wang, F. Huang and Y. Chen.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
		<br>
		<a href="https://arxiv.org/pdf/1811.00342.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://drive.google.com/open?id=1bDxP_i2ETfoO4uAbPhukCcxvOrVQuwd-"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://drive.google.com/open?id=1cFyjZWdGOBZ8t-63bZehERMaKpTkawwe"; style="color: #EE7F2D;">Supp</a>
		/
		<a href="https://github.com/tyshiwo/FHR_alignment"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FHR_alignment.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>		
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DAML_aaai19.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"><papertitle><b>Data-Adaptive Metric Learning with Scale Alignment</b></papertitle></a><br>
		S. Chen, C. Gong, J. Yang, <b>Y. Tai</b>, L. Hui and J. Li.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
		<br>
		<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"; style="color: #EE7F2D;">Paper (Official)</a>
		<br>
	    </p>
		</td></tr>						


		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CD_ECCV18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"><papertitle><b>Person Search via A Mask-Guided Two-Stream CNN Model</b></papertitle></a><br>
		D. Chen, S. Zhang, W. Ouyang, J. Yang and <b>Y. Tai</b>.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2018
		<br>
		<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://drive.google.com/file/d/1Wg4ieK4BQc5XMZCBPpQJXumoHs21AHBM/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		<br>
	    </p>
		</td></tr>	
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <a href="./projects/faceSR.gif"><img src="./projects/faceSR.gif" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1711.10703.pdf"><papertitle><b>FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</b></papertitle></a><br>
		<b>Y. Tai*</b>, Y. Chen*, X. Liu, C. Shen, J. Yang.
		<br>
                <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2018 <b style="color: red">[Spotlight]</b>
		<br>
		<a href="https://drive.google.com/file/d/10i2NZfUyf2Yold4ABusz3Que-XN_gEEu/view"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://arxiv.org/pdf/1711.10703.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://github.com/tyshiwo/FSRNet"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FSRNet.svg" alt="GitHub stars" title="" />
		/
		<a href="https://www.youtube.com/watch?v=RgHum6TB5aM"; style="color: #EE7F2D;">Demo</a>
		/
		<a href="https://drive.google.com/open?id=1U117O2ZyBDKVpPL06JA3g3uFtL6vDsp0"; style="color: #EE7F2D;">Slides</a>
		/
		<a href="https://drive.google.com/file/d/1JaSUxohFUz8m9ukAFzCbiT281nSG5lkY/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		
	    </p>
		</td></tr>
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		 <img src="./projects/MemNet_iccv17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"><papertitle><b>MemNet: A Persistent Memory Network for Image Restoration</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu, C. Xu.
		<br>
                <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2017 <b style="color: red">[Spotlight]</b>
		<br>
		<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/tyshiwo/MemNet"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/MemNet.svg" alt="GitHub stars" title="" />
		/
		<a href="https://drive.google.com/open?id=1maokCHq_EzOzft8UtaAFOOUzpgE6ehuk"; style="color: #EE7F2D;">Poster</a>
	        /
	        <a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/"; style="color: #EE7F2D;"> <b>'15 most influential papers' in ICCV 2017 by PaperDigest</b></a>
		</p>
                <p><font color="red">My second paper that achieves over 1,500 google scholar citations</font></p> 
	        </p>
		</td></tr>					
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		 <img src="./projects/DRRN_cvpr17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"><papertitle><b>Image Super-Resolution via Deep Recursive Residual Network</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu.
		<br>
                <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2017 
		<br>
		<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/tyshiwo/DRRN_CVPR17"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/DRRN_CVPR17.svg" alt="GitHub stars" title="" />
		/
		<a href="http://cvlab.cse.msu.edu/project-super-resolution.html"; style="color: #EE7F2D;">Project</a>
		/
		<a href="https://drive.google.com/file/d/1rWtZL3rQK5uzlw5lyPabop7MrbE0KS7j/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		
	    </p>
               <p><font color="red">My first paper that achieves over 2,000 google scholar citations</font></p> 
	    </p>
		</td></tr>	


        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/NMR_pami16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"><papertitle><b>Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes</b></papertitle></a><br>
		J. Yang, L. Luo, J. Qian, <b>Y. Tai</b>, F. Zhang and Y. Xu.
		<br>
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence</em>, 2017
		<br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>	
		
				
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SOPR_SDM16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"><papertitle><b>Structural Orthogonal Procrustes Regression for Face Recognition with Pose Variations and Misalignment</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, F. Zhang, Y. Zhang, L. Luo, J. Qian.
		<br>
        <em>SIAM Conference on Data Mining (<b>SDM</b>)</em>, 2016 <b style="color: red">[Oral]</b> 
		<br>
		<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>		
		
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/OPR_TIP16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"><papertitle><b>Face Recognition with Pose Variations and Misalignment via Orthogonal Procrustes Regression</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, Y. Zhang, L. Luo, J. Qian and Y. Chen
		<br>
                <em>IEEE Trans. on Image Processing</em>, 2016
		<br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>		
		
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/LDSVDR_PR2016.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"><papertitle><b>Learning Discriminative Singular Value Decomposition Representation for Face Recognition</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, L. Luo, F. Zhang and J. Qian
		<br>
       		<em>Pattern Recognition</em>, 2016
		<br>
		<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>				
						       
</table></p>
		
	<p style="text-align:left"><strong><font size="4px">Awards</font></strong></p>
		<ul>
		    <li><p style="text-align:left">2021 First Prize of Progress in Science and Technology of Jiangsu Province (4/11), <a href="http://www.jiangsu.gov.cn/module/download/downfile.jsp?classid=0&filename=d926727e803e49b788d4d7e909a5949e.pdf"; style="color: #EE7F2D;"> <b>“Image restoration and robust recognition: theory and algorithms”</b></a></p></li>
		    <li><p style="text-align:left">2021 Winner of CVPR NTIRE 2021 Challenge on Video Super-Resolution: Spatial-Temporal (Team name: Imagination)</p></li>
		    <li><p style="text-align:left">2020 Winner of CVPR NTIRE 2020 Challenge on Real-World Super-Resolution (Team name: Impressionism)</p></li>
		    <li><p style="text-align:left">2018 Stars of Youtu Lab, Tencent </p></li>
			<li><p style="text-align:left">2018, 2019, 2020, 2022 Outstanding Staff Award, Tencent </p></li>
			<li><p style="text-align:left">2018 Excellent Doctoral Dissertation of Nanjing University of Science and Technology, China </p></li>
			<li><p style="text-align:left">ICCV'17 Student Volunteer Travel Award </p></li>
			<li><p style="text-align:left">2017 Outstanding Graduate </p></li>
			<li><p style="text-align:left">2016 National Graduate Scholarship </p></li>
		</ul><br>

	<p style="text-align:justify"><strong><font size="4px">Professional activities</font></strong></p>
		<ul>
			<li><p style="text-align:left">Area Chairs for <a href="https://eccv2022.ecva.net/organizers/"; style="color: #EE7F2D;"> <b>ECCV 2022</b></a>, <a href="https://wacv2023.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2023</b></a>, <a href="http://fg2023.ieee-biometrics.org/organizers/"; style="color: #EE7F2D;"> <b>FG 2023</b></a> </p></li>
			<li><p style="text-align:left">Reviewer for CVPR'(17, 18, 19, 20, 21, 22), ICCV'(17, 19, 21), ECCV'(18, 20), AAAI'(19, 20, 21, 22), ICLR'(20, 21), NIPS'(20, 21) </p></li>
			<li><p style="text-align:left">Reviewer for Trans. on Pattern Analysis and Machine Intelligence (TPAMI), International Journal of Computer Vision (IJCV), IEEE Trans. on Image Processing (TIP), Pattern Recognition, Pattern Recognition Letters </p></li>
	    </ur>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=faf9f9&w=300&t=tt&d=hvoHWGKZcyFl6zhd6aLhpusD9f4jQY_gzPG8UfsmW0I&co=1285d6'></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
              Last modified in May 2023.
              For the style of my personal website, Please refer to the wonderful page from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              <br>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>          

</html>
