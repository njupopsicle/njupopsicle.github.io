<!doctype html>
<html>
<head>
<script async defer src="https://buttons.github.io/buttons.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108083124-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108083124-1');
</script>

<meta charset="UTF-8">
<title>Ying Tai (邰颖)</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta property="og:type" content="website">
<meta property="og:url" content="/var/www/html/index.html">
<meta property="og:site_name" content="Ying Tai">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ying Tai">
	
	
<link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
	
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha3841q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
	
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">
	
<link rel="stylesheet" href="styles.css">

</head>
  
<style type="text/css">
    body {
        background-color:#DDDDDD;
        font-family:georgia,sans-serif;
        line-height:120%;
        margin:auto;
        text-align:left;
        width:1000px;
    }
    .main {
        background-color:#FFFFFF;
        padding:20px 20px 20px 20px;
    }
    h2 {
        font-family:georgia,sans-serif;
        font-size:26px;
        letter-spacing:1px;
        margin-bottom:0;
    }
    h3 {
        font-family:georgia,sans-serif;
        font-size:20px;
        letter-spacing:1px;
    }
    a, a:visited {
        color:#0000EE;
        outline:none;
        text-decoration:none;
    }
    a:hover{
        text-decoration:underline;
    }
    .thumbs {
        width:800px;
        margin-left:auto;
        margin-right:auto;
        text-align:justify;
        text-justify:distribute-all-lines;
    }
    #thumbs a {
        vertical-align:center;
        display:inline-block;
        *display:inline;
    }
    .stretch {
        width:100%;
        display:inline-block;
        font-size:0;
        line-height:0
    }
</style>

<body>
	
<div class="container">
<div class="blog-header">
</div>
<div class="main">  
<article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
	
	<div class="article-inner" style="width:95%">
	<div class="article-entry" itemprop="articleBody">
	<table width="600">
        <tbody>
        <tr>
            <td align="center" width="200"><img src="./projects/dog&me.jpg" height="150" alt="Luke"/><br>My cute dog: Luke!</td>
            <td align="left" nowrap="">
                <dl>
                    <dd>
                        <h2>Ying Tai (邰颖)</h2><br>
                    </dd>
                    <dd>
                        Team Leader <br>
                    </dd>
                    <dd>
                        Youtu Lab, Tencent (Shanghai) <br><br>
                    </dd>
                    <dd>
                        Email: <a href="mailto:yingtai@tencent.com">yingtai@tencent.com</a>, <a href="mailto:tyshiwo@gmail.com">tyshiwo@gmail.com</a><br>
                    </dd>
                    <dd>
			<a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en" target="_blank" rel="external">[Google Scholar]</a>
			<a href="https://github.com/tyshiwo" target="_blank" rel="external">[Github]</a>
			<a href="https://www.linkedin.com/in/ying-tai-33a76ab6/" target="_blank" rel="external">[LinkedIn]</a>
                        <br><br>
                    </dd>
                </dl>
            </td>
        </tr>
        </tbody>
        </table>
      <hr>

      <p style="text-align:justify">I got my Ph.D. degree from the Department of Computer Science and Engineering, Nanjing University of Science and Technology (NUST) in 2017, and my advisor is Prof. <a href="http://www.patternrecognition.cn/~jian/">Jian Yang</a>. In 2016, I spent 6 wonderful months as a visiting student at Prof. <a href="http://www.cse.msu.edu/~liuxm/">Xiaoming Liu</a>'s lab in Michigan State University. 
      From Sep. 2017 to Sep. 2018, I was a Researcher at <a href="https://bestimage.qq.com/"> Tencent Youtu Lab</a>. Currently, I am a Senior Researcher at <a href="https://bestimage.qq.com/"> Tencent Youtu Lab</a>. 
      <br></p>
		
      <p style="text-align:left; color:red">We recently released codes on detection, alignment and attribute classification in Youtu Lab's official github account <a href="https://github.com/TencentYoutuResearch" target="_blank" rel="external">[TencentYoutuResearch]</a>, which currently has over <b>3.0K stars</b> and <b>750 forks</b>.
      </p>
		
      <br><br>
	
		<p style="text-align:left"><strong><font size="4px">Recent news</font></strong></p> 
		<ul>
		  <li><p style="text-align:left; color:red"><b>We are hiring interns on image generation and editing, image restoration and image segmentation. Feel free to send me (<a href="mailto:yingtai@tencent.com">yingtai@tencent.com</a>) your CV if you are interested.</b></p></li>
		  <li><p style="text-align:left">03/2021 – 3 papers accepted by CVPR'21 (1 Oral and 2 Posters), with the acceptance rate to be <b>23.7%</b> </p></li>
		  <li><p style="text-align:left">12/2020 – 4 papers accepted by AAAI'21, with the acceptance rate to be <b>21%</b> </p></li>
		  <li><p style="text-align:left">09/2020 – Training codes of RealSR are available in Tencent official github account <a href="https://github.com/Tencent/Real-SR" target="_blank" rel="external">[Tencent-RealSR]</a>. </p></li>
		  <li><p style="text-align:left">07/2020 – 6 papers accepted by ECCV'20, with the acceptance rate to be <b>27%</b> </p></li>
		  <li><p style="text-align:left">05/2020 – Our RealSR model (Team name: Impressionism) won both tracks of <a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a> </p></li>
		  <li><p style="text-align:left">02/2020 – 3 papers accepted by CVPR'20, with the acceptance rate to be <b>22.1%</b> </p></li>
	          <li><p style="text-align:left">11/2019 – 2 papers (Action Proposal & Action Recognition) accepted by AAAI'20, with the acceptance rate to be <b>20.6%</b>. The code of our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> is released at <a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG" target="_blank" rel="external">[ActionDetection-DBG]</a>, which achieves <b>Top 1</b> performance on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a> </p></li>
	          <li><p style="text-align:left">03/2019 – The evaluation code of our <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>DSFD</b></a> is released at <a href="https://github.com/TencentYoutuResearch/FaceDetection-DSFD" target="_blank" rel="external">[FaceDetection-DSFD]</a> </p></li>
	          <li><p style="text-align:left">02/2019 – Our <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>DSFD</b></a> on face detection is accepted by CVPR'19, with the acceptance rate to be <b>25.2%</b> </p></li>
                  <li><p style="text-align:left">11/2018 – 2 papers (face alignement & adaptive metric learning) accepted by AAAI'19, with the acceptance rate to be ONLY <b>16.2%</b> </p></li>
                  <li><p style="text-align:left">10/2018 – We released a novel <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>Dual Shot Face Detector (DSFD) </b> </a> framework that achieves <b>Top 1</b> performance on all FIVE settings of <b>WIDER FACE (Easy/Medium/Hard)</b> and <b>FDDB (Discrete/Continuous)</b> datasets </p></li>
                  <li><p style="text-align:left">09/2018 – Be a <b>Senior Researcher</b> at Tecent Youtu Lab </p></li>
                  <li><p style="text-align:left">09/2018 – <b>Reviewer</b> of CVPR'19 </p></li>
                  <li><p style="text-align:left">07/2018 – <b>Program committee</b> of AAAI'19 </p></li>
	          <li><p style="text-align:left">07/2018 – 1 paper accepted by ECCV'18 </p></li>
	          <li><p style="text-align:left">02/2018 – 1 paper accepted by CVPR'18 （SPOTLIGHT Presentation）</p></li>
                  <li><p style="text-align:left">09/2017 – Be a <b>Researcher</b> at Tecent Youtu Lab </p></li>
	          <li><p style="text-align:left">08/2017 – Received ICCV'17 Student Volunteer Travel Award </p></li>	
	          <li><p style="text-align:left">07/2017 – 1 paper accepted by ICCV'17 (SPOTLIGHT Presentation) </p></li>
	          <li><p style="text-align:left">03/2017 – 1 paper accepted by CVPR'17 </p></li>
	        </ul><br><br>
		
		<p style="text-align:justify"><strong><font size="4px">Preprints</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/Arxiv20-Style.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf"><b>Collaborative Learning for Faster StyleGAN Embedding</b></a><br>
		S. Guan, <b>Y. Tai</b>, B. Ni, F. Zhu, F. Huang and X. Yang.
		<br>
        	arXiv:2007.01758v1
		<br>
		<a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>	
			
			
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/ASFD_arXiv20.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/2003.11228.pdf"><b>ASFD: Automatic and Scalable Face Detector</b></a><br>
		B. Zhang*, Jian Li*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, Y. Xia, W. Pei and R. Ji.
		<br>
        	arXiv:2003.11228
		<br>
		<font color="red">Improved version of our CVPR'19 work DSFD</font>
		<br>
		<font color="red">Ranked No. 1 again on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a></font>
		<br>
		<a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>	
	        
		
	
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/AG_arxiv19.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/1902.10311.pdf"><b>Aurora Guard: Real-Time Face Anti-Spoofing via Light Reflection</b></a><br>
		Y. Liu, <b>Y. Tai</b>, J. Li, S. Ding, C. Wang, F. Huang, D. Li, W. Qi and R. Ji.
		<br>
        	arXiv:1902.10311v1, 2019
		<br>
		<a href="https://arxiv.org/pdf/1902.10311.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://drive.google.com/open?id=1wLXwGvy2zsh5xkDhRCPNzQXGUihZNq1g"; style="color: #EE7F2D;">[Demo]</a>
	        </p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
	
		</table>
		</p><br><br>
			
		<p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/3DFace_CVPR2021.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                <p><a href="google.com"><b>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</b></a><br>
		Z. Zhang, Y. Ge, R. Chen, <b>Y. Tai</b>, Y. Yan, J. Yang, C. Wang, J. Li, and F. Huang.
		<br>
        	Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021 <b style="color: red">[Oral]</b>
		<br>
		<a href="google.com"; style="color: #EE7F2D;">[Paper (Coming soon)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/AFSD_CVPR2021.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                <p><a href="google.com"><b>Learning Salient Boundary Feature for Anchor-free Temporal Action Localization</b></a><br>
		C. Lin*, C. Xu*, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.
		<br>
        	Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021
		<br>
		<a href="google.com"; style="color: #EE7F2D;">[Paper (Coming soon)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/REVIDE_CVPR2021.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                <p><a href="google.com"><b>Learning to Restore Hazy Video: A New Real-World Dataset and A New Method</b></a><br>
		X. Zhang, H. Dong, J. Pan, C. Zhu, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and F. Wang.
		<br>
        	Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021
		<br>
		<a href="google.com"; style="color: #EE7F2D;">[Paper (Coming soon)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/FCA_AAAI2021.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                <p><a href="https://arxiv.org/pdf/2012.10102.pdf"><b>Frequency Consistent Adaptation for Real World Super Resolution</b></a><br>
		X. Ji*, G. Tao*, Y. Cao, <b>Y. Tai</b>, T. Lu, C. Wang, J. Li, and F. Huang.
		<br>
        	The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021
		<br>
		<font color="red">Improved version of our prior work RealSR</font>
		<br>
		<a href="https://arxiv.org/pdf/2012.10102.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/CMR_AAAI2021.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                <p><a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"><b>Learning Comprehensive Motion Representation for Action Recognition</b></a><br>
		M. Wu*, B. Jiang*, D. Luo, J. Yan, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, and X. Yang.
		<br>
        	The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021
		<br>
	        <font color="red">Extented version of our prior works TEINet and TDRL</font>
		<br>
		<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"; style="color: #EE7F2D;">[Paper (Official)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/D2AM_AAAI2021.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                <p><a href="https://arxiv.org/pdf/2005.01996.pdf"><b>Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</b></a><br>
		Z. Chen, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, F. Huang, and X. Jin.
		<br>
        	The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021
		<br>
		<a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;">[Paper (Coming soon)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>			

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/SASNet_AAAI2021.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                <p><a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"><b>To Choose or to Fuse? Scale Selection for Crowd Counting</b></a><br>
		Q. Song*, C. Wang*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, J. Wu, and J. Ma.
		<br>
        	The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021
		<br>
		<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"; style="color: #EE7F2D;">[Paper (Official)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/FAN_ArXiv19.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/1911.11680.pdf"><b>FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization</b></a><br>
		X. Yin, <b>Y. Tai</b>, Y. Huang and X. Liu.
		<br>
        	Asian Conference on Computer Vision (<b>ACCV</b>), 2020
		<br>
		<font color="red">Novel framework to improve surveillance face recognition & normalization from unpaired data</font>
		<br>
		<a href="https://arxiv.org/pdf/1911.11680.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/DDL_Arxiv2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="https://arxiv.org/pdf/2002.03662.pdf"><b>Improving Face Recognition from Hard Samples via Distribution Distillation Loss</b></a><br>
		Y. Huang*, P. Shen*, <b>Y. Tai</b><sup>#</sup>, S. Li<sup>#</sup>, X. Liu, J. Li, F. Huang, and R. Ji.
		<br>
        	European Conference on Computer Vision (<b>ECCV</b>), 2020
		<br>
		<font color="red">Novel *practical* framework to improve hard examples for face recognition</font>
		<br>
		<a href="https://arxiv.org/pdf/2002.03662.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750137.pdf"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://github.com/HuangYG123/DDL"; style="color: #EE7F2D;">[Code (Official)]</a>
	        <br>
		<a class="github-button" href="https://github.com/HuangYG123/DDL" data-icon="octicon-star" data-show-count="true" aria-label="Star HuangYG123/DDL on GitHub">Star</a>
		<a class="github-button" href="https://github.com/HuangYG123/DDL/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork HuangYG123/DDL on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/SSCGAN_ECCV2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"><b>SSCGAN: Facial Attribute Editing via Style Skip Connections</b></a><br>
		W. Chu, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, F. Huang, and R. Ji.
		<br>
        	European Conference on Computer Vision (<b>ECCV</b>), 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"; style="color: #EE7F2D;">[Paper (Oficial)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/DRL_ECCV2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/2008.08250.pdf"><b>Face Anti-Spoofing via Disentangled Representation Learning</b></a><br>
		K. Zhang, T. Yao, J. Zhang, <b>Y. Tai</b><sup>#</sup>, S. Ding, J. Li, F. Huang, H. Song and L. Ma.
		<br>
        	European Conference on Computer Vision (<b>ECCV</b>), 2020 
		<br>
		<a href="https://arxiv.org/pdf/2008.08250.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640630.pdf"; style="color: #EE7F2D;">[Paper (Official)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/CTracker_ECCV2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/2007.14557.pdf"><b>Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End <br> Joint Multiple-Object Detection and Tracking</b></a><br>
		J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.
		<br>
        	European Conference on Computer Vision (<b>ECCV</b>), 2020 <b style="color: red">[SPOTLIGHT]</b>
		<br>
		<a href="https://arxiv.org/pdf/2007.14557.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490137.pdf"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://github.com/pjl1995/CTracker"; style="color: #EE7F2D;">[Code (Official)]</a>
		<br>
		<a class="github-button" href="https://github.com/pjl1995/CTracker" data-icon="octicon-star" data-show-count="true" aria-label="Star pjl1995/CTracker on GitHub">Star</a>
		<a class="github-button" href="https://github.com/pjl1995/CTracker/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork pjl1995/CTracker on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/TDRL_ECCV2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/2007.07626.pdf"><b>Temporal Distinct Representation Learning for 2D-CNN-based Action Recognition</b></a><br>
		J. Weng, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, X. Jiang and J. Yuan.
		<br>
        	European Conference on Computer Vision (<b>ECCV</b>), 2020 
		<br>
		<a href="https://arxiv.org/pdf/2007.07626.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520358.pdf"; style="color: #EE7F2D;">[Paper (Official)]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/HPE_arXiv2019.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/2008.00697.pdf"><b>Adversarial Semantic Data Augmentation for Human Pose Estimation</b></a><br>
		Y. Bin, X. Cao, X. Chen, Y. Ge, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, C. Gao and N. Sang.
		<br>
        	European Conference on Computer Vision (<b>ECCV</b>), 2020
		<br>
		<font color="red">State-of-the-art performance on <a href="http://human-pose.mpi-inf.mpg.de/#results"; style="color: #EE7F2D;"> <b>MPII</b></a></font> and <a href="http://sam.johnson.io/research/lsp.html"; style="color: #EE7F2D;"> <b>LSP</b></a></font>
		<br>
		<a href="https://arxiv.org/pdf/2008.00697.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640596.pdf"; style="color: #EE7F2D;">[Paper (Official)]</a>	
		<a href="https://github.com/Binyr/ASDA"; style="color: #EE7F2D;">[Code (Official)]</a>
		<br>
		<a class="github-button" href="https://github.com/Binyr/ASDA" data-icon="octicon-star" data-show-count="true" aria-label="Star Binyr/ASDA on GitHub">Star</a>
		<a class="github-button" href="https://github.com/Binyr/ASDA/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork Binyr/ASDA on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
	
	
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/RealSR_CVPRW2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="https://arxiv.org/pdf/2005.01996.pdf"><b>Real-World Super-Resolution via Kernel Estimation and Noise Injection</b></a><br>
		X. Ji, Y. Cao, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, and F. Huang.
		<br>
        	Computer Vision and Pattern Recognition Workshop (<b>CVPRW</b>), 2020
		<br>
		<font color="red">Winner of <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a> (Team name: Impressionism)</font>
		<br>
		<font color="red">Provide a practical and general solution for real image SR with NO need of paired data</font>
		<br>
		<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;">[Paper]</a>
		<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">[Code (Official)]</a>
		<a href="https://github.com/nihui/realsr-ncnn-vulkan"; style="color: #EE7F2D;">[Code (NCNN-vulkan)]</a>
		<a href="https://openbenchmarking.org/test/pts/realsr-ncnn"; style="color: #EE7F2D;">[OpenBenchmarking.org]</a>
		<a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;">[Official Challenge Report]</a>
		<br>
		<a class="github-button" href="https://github.com/jixiaozhong/RealSR" data-icon="octicon-star" data-show-count="true" aria-label="Star jixiaozhong/RealSR on GitHub">Star</a>
		<a class="github-button" href="https://github.com/jixiaozhong/RealSR/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork jixiaozhong/RealSR on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
			
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/CurricularFace_CVPR2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"><b>CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</b></a><br>
		Y. Huang, Y. Wang, <b>Y. Tai</b><sup>#</sup>, X. Liu, P. Shen, S. Li<sup>#</sup>, J. Li, and F. Huang.
		<br>
        	Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020
		<br>
		<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://github.com/HuangYG123/CurricularFace"; style="color: #EE7F2D;">[Code]</a>
	        <br>
		<a class="github-button" href="https://github.com/HuangYG123/CurricularFace" data-icon="octicon-star" data-show-count="true" aria-label="Star HuangYG123/CurricularFace on GitHub">Star</a>
		<a class="github-button" href="https://github.com/HuangYG123/CurricularFace/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork HuangYG123/CurricularFace on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/UnsupervisedOpticalFlow_CVPR2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"><b>Learning by Analogy: Reliable Supervision from Transformations for <br> Unsupervised Optical Flow Estimation</b></a><br>
		L. Liu, J. Zhang, Y. Liu, Y. Wang, <b>Y. Tai</b>, D. Luo, C. Wang, J. Li, and F. Huang.
		<br>
        	Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020
		<br>
		<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://github.com/lliuz/ARFlow"; style="color: #EE7F2D;">[Code]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/VideoReID_CVPR2020.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"><b>Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</b></a><br>
		Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, <b>Y. Tai</b>, and L. Shao.
		<br>
        	Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020
		<br>
		<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://github.com/daodaofr/hypergraph_reid"; style="color: #EE7F2D;">[Code]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/DBG_AAAI20.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/1911.04127.pdf"><b>Fast Learning of Temporal Action Proposal via Dense Boundary Generator</b></a><br>
		C. Lin*, J. Li*, Y. Wang, <b>Y. Tai</b>, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang and R. Ji.
		<br>
        	The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2020 (Acceptance Rate: <b>20.6%</b>)
		<br>
		<font color="red">Ranked No. 1 on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a></font>
		<br>
		<a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG"; style="color: #EE7F2D;">[Code]</a>
		<br>
		<a class="github-button" href="https://github.com/Tencent/ActionDetection-DBG" data-icon="octicon-star" data-show-count="true" aria-label="Star TencentYoutuResearch/ActionDetection-DBG on GitHub">Star</a>
		<a class="github-button" href="https://github.com/Tencent/ActionDetection-DBG/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork TencentYoutuResearch/ActionDetection-DBG on GitHub">Fork</a>
	        </p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>

		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/TEI_AAAI20.png" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/1911.09435.pdf"><b>TEINet: Towards an Efficient Architecture for Video Recognition</b></a><br>
		Z. Liu*, D. Luo*, Y. Wang, L. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and T. Lu.
		<br>
        	The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2020 (Acceptance Rate: <b>20.6%</b>)
		<br>
		<a href="https://arxiv.org/pdf/1911.09435.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
	        </p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
			
                <tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/face_detection_arxiv18.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://128.84.21.199/abs/1810.10220"><b>DSFD: Dual Shot Face Detector</b></a><br>
		J. Li, Y. Wang, C. Wang, <b>Y. Tai</b>, J. Qian, J. Yang, C.e Wang, J. Li and F. Huang.
		<br>
        	Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019
		<br>
		<font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a> and <a href="http://vis-www.cs.umass.edu/fddb/results.html#eval"; style="color: #EE7F2D;"> <b>FDDB</b></a> (Until 2019.01)</font>
		<br>
		<a href="https://drive.google.com/open?id=1DuhcD4Thwv8E3kcLZIOQTeo8qaGDq9PB"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://github.com/TencentYoutuResearch/FaceDetection-DSFD"; style="color: #EE7F2D;">[Code]</a>
		<br>
		<a class="github-button" href="https://github.com/Tencent/FaceDetection-DSFD" data-icon="octicon-star" data-show-count="true" aria-label="Star TencentYoutuResearch/FaceDetection-DSFD on GitHub">Star</a>
		<a class="github-button" href="https://github.com/Tencent/FaceDetection-DSFD/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork TencentYoutuResearch/FaceDetection-DSFD on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
          
 		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/aaai19_FHR.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://arxiv.org/pdf/1811.00342.pdf"><b>Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos</b></a><br>
		<b>Y. Tai*</b>, Y. Liang*, X. Liu, L. Duan, J. Li, C. Wang, F. Huang and Y. Chen.
		<br>
            The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2019 (Acceptance Rate: <b>16.2%</b>)
		<br>
		<a href="https://drive.google.com/open?id=1bDxP_i2ETfoO4uAbPhukCcxvOrVQuwd-"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://arxiv.org/pdf/1811.00342.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://drive.google.com/open?id=1cFyjZWdGOBZ8t-63bZehERMaKpTkawwe"; style="color: #EE7F2D;">[Supp]</a>
                <a href="https://github.com/tyshiwo/FHR_alignment"; style="color: #EE7F2D;">[Code]</a>
		<br>
		<a class="github-button" href="https://github.com/TencentYoutuResearch/FaceAlignment-FHR" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/FHR_alignment on GitHub">Star</a>
		<a class="github-button" href="https://github.com/TencentYoutuResearch/FaceAlignment-FHR/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/FHR_alignment on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>

          <tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/DAML_aaai19.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
          <p><a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"><b>Data-Adaptive Metric Learning with Scale Alignment</b></a><br>
		S. Chen, C. Gong, J. Yang, <b>Y. Tai</b>, L. Hui and J. Li.
		<br>The AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2019 (Acceptance Rate: <b>16.2%</b>)
		<br>
		<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"; style="color: #EE7F2D;">[Paper (Official)]</a>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
          
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/CD_ECCV18.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
        <p><a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"><b>Person Search via A Mask-Guided Two-Stream CNN Model</b></a><br>
		D. Chen, S. Zhang, W. Ouyang, J. Yang and <b>Y. Tai</b>.
		<br>
                European Conference on Computer Vision (<b>ECCV</b>), 2018
		<br>
		<b><i>IEEE Trans. on Image Processing</i></b>, 2020
		<br>
		<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"; style="color: #EE7F2D;">[Paper]</a>
		<a href="https://drive.google.com/file/d/1Wg4ieK4BQc5XMZCBPpQJXumoHs21AHBM/view?usp=sharing"; style="color: #EE7F2D;">[Poster]</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
			
	    <tr style="border-width: 1px">
		<td style="border: 0px;"><a href="./projects/faceSR.gif"><img src="./projects/faceSR.gif" style="height: 95px; width: 200px"></a></td>
		<td style="border: 0px;"> 
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="https://arxiv.org/pdf/1711.10703.pdf"><b>FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</b></a><br>
		Y. Chen*, <b>Y. Tai*</b>, X. Liu, C. Shen, J. Yang.<br>
		Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018 <b style="color: red">[SPOTLIGHT]
		</b><br>
		<a href="https://drive.google.com/file/d/10i2NZfUyf2Yold4ABusz3Que-XN_gEEu/view"; style="color: #EE7F2D;">[Paper (Official)]</a>
		<a href="https://arxiv.org/pdf/1711.10703.pdf"; style="color: #EE7F2D;">[Paper (arXiv)]</a>
		<a href="https://github.com/tyshiwo/FSRNet"; style="color: #EE7F2D;">[Code]</a>
		<a href="https://www.youtube.com/watch?v=RgHum6TB5aM"; style="color: #EE7F2D;">[Demo]</a>
		<a href="https://drive.google.com/open?id=1U117O2ZyBDKVpPL06JA3g3uFtL6vDsp0"; style="color: #EE7F2D;">[Slides]</a>
		<a href="https://drive.google.com/file/d/1JaSUxohFUz8m9ukAFzCbiT281nSG5lkY/view?usp=sharing"; style="color: #EE7F2D;">[Poster]</a>
		<br>
		<a class="github-button" href="https://github.com/tyshiwo/FSRNet" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/FSRNet on GitHub">Star</a>
		<a class="github-button" href="https://github.com/tyshiwo/FSRNet/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/FSRNet on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/MemNet_iccv17.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"><b>MemNet: A Persistent Memory Network for Image Restoration</b></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu, C. Xu.
		<br>
                International Conference on Computer Vision (<b>ICCV</b>), 2017 <b style="color: red">[SPOTLIGHT]</b><br> 
		<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"; style="color: #EE7F2D;">[Paper]</a>
		<a href="https://github.com/tyshiwo/MemNet"; style="color: #EE7F2D;">[Code]</a>
		<a href="https://drive.google.com/open?id=1maokCHq_EzOzft8UtaAFOOUzpgE6ehuk"; style="color: #EE7F2D;">[Poster]</a>
		<br>
		<a class="github-button" href="https://github.com/tyshiwo/MemNet" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/MemNet on GitHub">Star</a>
		<a class="github-button" href="https://github.com/tyshiwo/MemNet/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/MemNet on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/DRRN_cvpr17.JPG" style="height: 95px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"><b>Image Super-Resolution via Deep Recursive Residual Network</b></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu.
		<br>
                Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017 <br> 
		<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"; style="color: #EE7F2D;">[Paper]</a>
		<a href="https://github.com/tyshiwo/DRRN_CVPR17"; style="color: #EE7F2D;">[Code]</a>
		<a href="http://cvlab.cse.msu.edu/project-super-resolution.html"; style="color: #EE7F2D;">[Project]</a>
		<a href="https://drive.google.com/file/d/1rWtZL3rQK5uzlw5lyPabop7MrbE0KS7j/view?usp=sharing"; style="color: #EE7F2D;">[Poster]</a>
		<br>
		<a class="github-button" href="https://github.com/tyshiwo/DRRN_CVPR17" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/DRRN_CVPR17 on GitHub">Star</a>
		<a class="github-button" href="https://github.com/tyshiwo/DRRN_CVPR17/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/DRRN_CVPR17 on GitHub">Fork</a>
		</p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/SOPR_SDM16.JPG" style="height: 95px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"><b>Structural Orthogonal Procrustes Regression for Face Recognition <br> with Pose Variations and Misalignment</b></a><br>
		<b>Y. Tai</b>, J. Yang, F. Zhang, Y. Zhang, L. Luo, J. Qian.
		<br>
        SIAM Conference on Data Mining (<b>SDM</b>), 2017 <br>
		<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"; style="color: #EE7F2D;">[Paper]</a></p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
	
        <tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/NMR_pami16.JPG" style="height: 95px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"><b>Nuclear Norm based Matrix Regression with Applications to Face Recognition <br> with Occlusion and Illumination Changes</b></a><br>
		J. Yang, L. Luo, J. Qian, <b>Y. Tai</b>, F. Zhang and Y. Xu.
		<br>
        <b><i>IEEE Trans. on Pattern Analysis and Machine Intelligence</i></b> 39(1): 156-171, 2017 <br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"; style="color: #EE7F2D;">[Paper]</a></p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>
		
		<tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/OPR_TIP16.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;">
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"><b>Face Recognition with Pose Variations and Misalignment <br> via Orthogonal Procrustes Regression</b></a><br>
		<b>Y. Tai</b>, J. Yang, Y. Zhang, L. Luo, J. Qian and Y. Chen
		<br>
		<b><i>IEEE Trans. on Image Processing</i></b> 25(6): 2673-2683, 2016 <br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"; style="color: #EE7F2D;">[Paper]</a></p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>		


        <tr style="border-width: 10px">
		<td style="border: 0px;"><img src="./projects/LDSVDR_PR2016.JPG" style="height: 90px; width: 200px; margin-top: 10px"></td>
		<td style="border: 0px;"> 
		<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
		<p><a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"><b>Learning Discriminative Singular Value Decomposition Representation <br> for Face Recognition</b></a><br>
		<b>Y. Tai</b>, J. Yang, L. Luo, F. Zhang and J. Qian
		<br>
		<b><i>Pattern Recognition</i></b> 50: 1-16, 2016 <br> 
		<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"; style="color: #EE7F2D;">[Paper]</a></p>
		</td><td style="width: 10px;border: 0px"></td></tr></table></td></tr>

		
		</table>
		</p><br><br>
		
		<p style="text-align:left"><strong><font size="4px">Awards</font></strong></p>
		<ul>
			<li><p style="text-align:left">2018 Stars of Youtu Lab, Tencent </p></li>
			<li><p style="text-align:left">2018, 2019, 2020 Outstanding Staff Award, Tencent </p></li>
			<li><p style="text-align:left">2018 Excellent Doctoral Dissertation of Nanjing University of Science and Technology, China </p></li>
			<li><p style="text-align:left">ICCV'17 Student Volunteer Travel Award </p></li>
			<li><p style="text-align:left">2017 Outstanding Graduate </p></li>
			<li><p style="text-align:left">2016 National Graduate Scholarship </p></li>
			<li><p style="text-align:left">2012 Top 10 Graduate Singers, Nanjing University of Science and Technology </p></li>
		</ul><br><br>
           
		
		<p style="text-align:justify"><strong><font size="4px">Professional activities</font></strong></p>
		<ul>
			<li><p style="text-align:left">Reviewer for CVPR'(17, 18, 19, 20, 21), ICCV'(17,19,21), ECCV'(18, 20), AAAI'(19, 20), ICLR'20, NIPS'20 </p></li>
			<li>
			  <p style="text-align:left">Reviewer for Trans. on Pattern Analysis and Machine Intelligence (TPAMI), International Journal of Computer Vision (IJCV), IEEE Trans. on Image Processing (TIP), Pattern Recognition, Pattern Recognition Letters </p></li>
	    </ur>

		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=faf9f9&w=300&t=tt&d=hvoHWGKZcyFl6zhd6aLhpusD9f4jQY_gzPG8UfsmW0I&co=1285d6'></script>
		
	</div>
	</div>
	</center>
	</article>
	<hr>
	</div>
	</div>
	
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="script.js"></script>
</body>
</html>
