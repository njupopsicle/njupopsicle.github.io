<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ying Tai(邰颖)</title>
  
  <meta name="author" content="Ying Tai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><strong><font size="4px">Ying Tai(邰颖)</font></strong></name>
              </p>
              <p>I am a Researcher and Team Lead at Tencent Youtu Lab, where I work on computer vision and machine learning.
              </p>
              <p>
              I got my Ph.D. degree from the Department of Computer Science and Engineering, Nanjing University of Science & Technology (NUST) in 2017, and my advisor is Prof. <a href="http://www.patternrecognition.cn/~jian/">Jian Yang</a>. In 2016, I spent 6 wonderful months as a visiting student at Prof. <a href="http://www.cse.msu.edu/~liuxm/">Xiaoming Liu</a>'s lab in Michigan State University. 
              <p>
              Most of my works have released codes in <a href="https://github.com/TencentYoutuResearch" target="_blank" rel="external">[TencentYoutuResearch]</a>, with over <b>4.0K stars</b> and <b>750 forks</b>.
              </p>
              <p style="text-align:center">
                <a href="mailto:tyshiwo@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/tyshiwo/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ying-tai-33a76ab6/">LinkedIn</a> 
              </p>
            </td>
            
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./projects/dog&me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./projects/dog&me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
<p style="text-align:left"><strong><font size="4px">Recent news</font></strong></p> 
		<ul>
		<li><p style="text-align:left">04/2021 – Our Team <b>Imagination</b> is the winner of CVPR NTIRE 2021 Challenge on Video Spatial-Temporal Super-Resolution</p></li>
		  <li><p style="text-align:left">03/2021 – 3 papers accepted by CVPR'21 (1 Oral and 2 Posters), with the acceptance rate to be <b>23.7%</b> </p></li>
		  <li><p style="text-align:left">12/2020 – 4 papers accepted by AAAI'21, with the acceptance rate to be <b>21%</b> </p></li>
		  <li><p style="text-align:left">09/2020 – Training codes of RealSR are available in Tencent official github account <a href="https://github.com/Tencent/Real-SR" target="_blank" rel="external">[Tencent-RealSR]</a>. </p></li>
		  <li><p style="text-align:left">07/2020 – 6 papers accepted by ECCV'20, with the acceptance rate to be <b>27%</b> </p></li>
		  <li><p style="text-align:left">05/2020 – Our RealSR model (Team name: Impressionism) won both tracks of <a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a> </p></li>
		  <li><p style="text-align:left">02/2020 – 3 papers accepted by CVPR'20, with the acceptance rate to be <b>22.1%</b> </p></li>
	          <li><p style="text-align:left">11/2019 – 2 papers (Action Proposal & Action Recognition) accepted by AAAI'20, with the acceptance rate to be <b>20.6%</b>. The code of our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> is released at <a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG" target="_blank" rel="external">[ActionDetection-DBG]</a>, which achieves <b>Top 1</b> performance on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a> </p></li>
	          <li><p style="text-align:left">02/2019 – Our <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>DSFD</b></a> on face detection is accepted by CVPR'19, with the acceptance rate to be <b>25.2%</b> </p></li>
                  <li><p style="text-align:left">11/2018 – 2 papers (face alignement & adaptive metric learning) accepted by AAAI'19, with the acceptance rate to be ONLY <b>16.2%</b> </p></li>
                  <li><p style="text-align:left">10/2018 – We released a novel <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>Dual Shot Face Detector (DSFD) </b> </a> framework that achieves <b>Top 1</b> performance on all FIVE settings of <b>WIDER FACE (Easy/Medium/Hard)</b> and <b>FDDB (Discrete/Continuous)</b> datasets </p></li>
	          <li><p style="text-align:left">07/2018 – 1 paper accepted by ECCV'18 </p></li>
	          <li><p style="text-align:left">02/2018 – 1 paper accepted by CVPR'18 （SPOTLIGHT Presentation）</p></li>	
	          <li><p style="text-align:left">07/2017 – 1 paper accepted by ICCV'17 (SPOTLIGHT Presentation) </p></li>
	          <li><p style="text-align:left">03/2017 – 1 paper accepted by CVPR'17 </p></li>
	        </ul>
	 
	<br>
	<p style="text-align:justify"><strong><font size="4px">Preprints</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
		
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/Arxiv20-Style.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf">
              <papertitle><b>Collaborative Learning for Faster StyleGAN Embedding</b></papertitle>
              </a>
              <br>
              S. Guan, <b>Y. Tai</b>, B. Ni, F. Zhu, F. Huang and X. Yang.              
              <br>
              <em>arXiv</em>, 2020 
              <br>
              <a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr>
            
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ASFD_arXiv20.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2003.11228.pdf">
              <papertitle><b>ASFD: Automatic and Scalable Face Detector</b></papertitle>
              </a>
              <br>
              B. Zhang*, Jian Li*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, Y. Xia, W. Pei and R. Ji.              
              <br>
              <em>arXiv</em>, 2020 
              <br>
              <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a></font></p>
            </td><tr>


            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AG_arxiv19.JPG" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.00713">
              <papertitle><b>Aurora Guard: Reliable Face Anti-Spoofing via Mobile Lighting System</b></papertitle>
              </a>
              <br>
              J. Zhang, <b>Y. Tai</b>, T. Yao, J. Meng, S. Ding, C. Wang, J. Li, F. Huang and R. Ji.             
              <br>
              <em>arXiv</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2102.00713"; style="color: #EE7F2D;">arXiv</a>
            
            </td><tr>
        </table></p>
 
 <p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/3DFace_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="google.com">
              <papertitle><b>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</b></papertitle>
              </a>
              <br>
              Z. Zhang, Y. Ge, R. Chen, <b>Y. Tai</b>, Y. Yan, J. Yang, C. Wang, J. Li, and F. Huang.              
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
              <br>
              <a href="google.com"; style="color: #EE7F2D;">arXiv</a>
              
            </td><tr>		
		
		           
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AFSD_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.13137">
              <papertitle><b>Learning Salient Boundary Feature for Anchor-free Temporal Action Localization</b></papertitle>
              </a>
              <br>
              C. Lin*, C. Xu*, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.              
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2103.13137"; style="color: #EE7F2D;">arXiv</a>
              
            </td><tr>	
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/REVIDE_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="google.com"><papertitle><b>Learning to Restore Hazy Video: A New Real-World Dataset and A New Method</b></papertitle></a><br>
		X. Zhang, H. Dong, J. Pan, C. Zhu, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and F. Wang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
		<br>
		<a href="google.com"; style="color: #EE7F2D;">Paper</a>
		
		</td></tr>

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/FCA_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2012.10102.pdf"><papertitle><b>Frequency Consistent Adaptation for Real World Super Resolution</b></papertitle></a><br>
		X. Ji*, G. Tao*, Y. Cao, <b>Y. Tai</b>, T. Lu, C. Wang, J. Li, and F. Huang.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021 
		<br>
		<a href="https://arxiv.org/pdf/2012.10102.pdf"; style="color: #EE7F2D;">arXiv</a>
		</p>
		<p><font color="red">Improved version of our prior work RealSR</font></p>
		</td></tr>			  
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CMR_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"><papertitle><b>Learning Comprehensive Motion Representation for Action Recognition</b></papertitle></a><br>
		M. Wu*, B. Jiang*, D. Luo, J. Yan, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, and X. Yang.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"; style="color: #EE7F2D;">arXiv</a>
		</p>
		<p><font color="red">Extented version of our prior works TEINet and TDRL</font></p>
		</td></tr>	


          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/D2AM_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="google.com"><papertitle><b>Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</b></papertitle></a><br>
		Z. Chen, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, F. Huang, and X. Jin.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="google.com"; style="color: #EE7F2D;">Paper(Coming soon)</a>
		</p>
		</td></tr>	

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SASNet_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"><papertitle><b>To Choose or to Fuse? Scale Selection for Crowd Counting</b></papertitle></a><br>
		Q. Song*, C. Wang*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, J. Wu, and J. Ma.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/FAN_ArXiv19.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.11680.pdf"><papertitle><b>FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization</b></papertitle></a><br>
		X. Yin, <b>Y. Tai</b>, Y. Huang and X. Liu.
		<br>
        <em>Asian Conference on Computer Vision (<b>ACCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.11680.pdf"; style="color: #EE7F2D;">Paper</a>
		</p>
		<p><font color="red">Novel framework to improve surveillance face recognition & normalization from unpaired data</font></p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DDL_Arxiv2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2002.03662.pdf"><papertitle><b>Improving Face Recognition from Hard Samples via Distribution Distillation Loss</b></papertitle></a><br>
		Y. Huang*, P. Shen*, <b>Y. Tai</b><sup>#</sup>, S. Li<sup>#</sup>, X. Liu, J. Li, F. Huang, and R. Ji.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/2002.03662.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/HuangYG123/DDL"; style="color: #EE7F2D;">Code (Official)</a>
		<br>
		<a class="github-button" href="https://github.com/HuangYG123/DDL" data-icon="octicon-star" data-show-count="true" aria-label="Star HuangYG123/DDL on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/HuangYG123/DDL/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork HuangYG123/DDL on GitHub">Fork</a>
		</p>
		</td></tr>				
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SSCGAN_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"><papertitle><b>SSCGAN: Facial Attribute Editing via Style Skip Connections</b></papertitle></a><br>
		W. Chu, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, F. Huang, and R. Ji.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>		           
            

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2008.08250.pdf"><papertitle><b>Face Anti-Spoofing via Disentangled Representation Learning</b></papertitle></a><br>
		K. Zhang, T. Yao, J. Zhang, <b>Y. Tai</b><sup>#</sup>, S. Ding, J. Li, F. Huang, H. Song and L. Ma.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640630.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CTracker_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2007.14557.pdf"><papertitle><b>Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End <br> Joint Multiple-Object Detection and Tracking</b></papertitle></a><br>
		J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/2007.14557.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/pjl1995/CTracker"; style="color: #EE7F2D;">Code (Official)</a>
		<br>
		<a class="github-button" href="https://github.com/pjl1995/CTracker" data-icon="octicon-star" data-show-count="true" aria-label="Star pjl1995/CTracker on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/pjl1995/CTracker/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork pjl1995/CTracker on GitHub">Fork</a>
	    </p>
		</td></tr>			

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/TDRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2007.07626.pdf"><papertitle><b>Temporal Distinct Representation Learning for 2D-CNN-based Action Recognition</b></papertitle></a><br>
		J. Weng, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, X. Jiang and J. Yuan.		    <br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520358.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/HPE_arXiv2019.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2008.00697.pdf"><papertitle><b>Adversarial Semantic Data Augmentation for Human Pose Estimation</b></papertitle></a><br>
		Y. Bin, X. Cao, X. Chen, Y. Ge, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, C. Gao and N. Sang.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/2008.00697.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640596.pdf"; style="color: #EE7F2D;">Paper (Official)</a>	
		/
		<a href="https://github.com/Binyr/ASDA"; style="color: #EE7F2D;">Code (Official)</a>
		<br>
		<a class="github-button" href="https://github.com/Binyr/ASDA" data-icon="octicon-star" data-show-count="true" aria-label="Star Binyr/ASDA on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/Binyr/ASDA/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork Binyr/ASDA on GitHub">Fork</a>	
	    </p>
	    <p><font color="red">State-of-the-art performance on <a href="http://human-pose.mpi-inf.mpg.de/#results"; style="color: #EE7F2D;"> <b>MPII</b></a></font> and <a href="http://sam.johnson.io/research/lsp.html"; style="color: #EE7F2D;"> <b>LSP</b></a></font></p>
		</td></tr>				
		

         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/RealSR_CVPRW2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2005.01996.pdf"><papertitle><b>Real-World Super-Resolution via Kernel Estimation and Noise Injection</b></papertitle></a><br>
		X. Ji, Y. Cao, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition Workshop (<b>CVPRW</b>)</em>, 2020
		<br>
		<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Official)</a>
		/
		<a href="https://github.com/nihui/realsr-ncnn-vulkan"; style="color: #EE7F2D;">Code (NCNN-vulkan)</a>
		/
		<a href="https://openbenchmarking.org/test/pts/realsr-ncnn"; style="color: #EE7F2D;">OpenBenchmarking.org</a>
		/
		<a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;">Challenge Report</a>
		<br>
		<a class="github-button" href="https://github.com/jixiaozhong/RealSR" data-icon="octicon-star" data-show-count="true" aria-label="Star jixiaozhong/RealSR on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/jixiaozhong/RealSR/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork jixiaozhong/RealSR on GitHub">Fork</a>	
	    </p>
	    <p><font color="red">Winner of <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a></font></p>
		</td></tr>				
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CurricularFace_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"><papertitle><b>CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</b></papertitle></a><br>
		Y. Huang, Y. Wang, <b>Y. Tai</b><sup>#</sup>, X. Liu, P. Shen, S. Li<sup>#</sup>, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/HuangYG123/CurricularFace"; style="color: #EE7F2D;">Code</a>
		<br>
		<a class="github-button" href="https://github.com/HuangYG123/CurricularFace" data-icon="octicon-star" data-show-count="true" aria-label="Star HuangYG123/CurricularFace on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/HuangYG123/CurricularFace/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork HuangYG123/CurricularFace on GitHub">Fork</a>
	    </p>
		</td></tr>			
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/UnsupervisedOpticalFlow_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"><papertitle><b>Learning by Analogy: Reliable Supervision from Transformations for <br> Unsupervised Optical Flow Estimation</b></papertitle></a><br>
		L. Liu, J. Zhang, Y. Liu, Y. Wang, <b>Y. Tai</b>, D. Luo, C. Wang, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/lliuz/ARFlow"; style="color: #EE7F2D;">Code</a>
		<br>
	    </p>
		</td></tr>	
	
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/VideoReID_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"><papertitle><b>Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</b></papertitle></a><br>
		Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, <b>Y. Tai</b>, and L. Shao.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/daodaofr/hypergraph_reid"; style="color: #EE7F2D;">Code</a>
		<br>
	    </p>
		</td></tr>	
		
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DBG_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.04127.pdf"><papertitle><b>Fast Learning of Temporal Action Proposal via Dense Boundary Generator</b></papertitle></a><br>
		C. Lin*, J. Li*, Y. Wang, <b>Y. Tai</b>, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang and R. Ji.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG"; style="color: #EE7F2D;">Code</a>
		<br>
		<a class="github-button" href="https://github.com/Tencent/ActionDetection-DBG" data-icon="octicon-star" data-show-count="true" aria-label="Star TencentYoutuResearch/ActionDetection-DBG on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/Tencent/ActionDetection-DBG/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork TencentYoutuResearch/ActionDetection-DBG on GitHub">Fork</a>
	    <p><font color="red">Ranked No. 1 on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a></font></p> 
	    </p>
		</td></tr>					
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/TEI_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.09435.pdf"><papertitle><b>TEINet: Towards an Efficient Architecture for Video Recognition</b></papertitle></a><br>
		Z. Liu*, D. Luo*, Y. Wang, L. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and T. Lu.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.09435.pdf"; style="color: #EE7F2D;">arXiv</a>
	    <br>	    
	    </p>
		</td></tr>	
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/face_detection_arxiv18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://128.84.21.199/abs/1810.10220"><papertitle><b>DSFD: Dual Shot Face Detector</b></papertitle></a><br>
		J. Li, Y. Wang, C. Wang, <b>Y. Tai</b>, J. Qian, J. Yang, C.e Wang, J. Li and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2019
		<br>
		<a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://drive.google.com/open?id=1DuhcD4Thwv8E3kcLZIOQTeo8qaGDq9PB"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/TencentYoutuResearch/FaceDetection-DSFD"; style="color: #EE7F2D;">Code</a>
		<br>
		<a class="github-button" href="https://github.com/Tencent/FaceDetection-DSFD" data-icon="octicon-star" data-show-count="true" aria-label="Star TencentYoutuResearch/FaceDetection-DSFD on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/Tencent/FaceDetection-DSFD/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork TencentYoutuResearch/FaceDetection-DSFD on GitHub">Fork</a>
	    <p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a> and <a href="http://vis-www.cs.umass.edu/fddb/results.html#eval"; style="color: #EE7F2D;"> <b>FDDB</b></a> (Until 2019.01)</font></p> 
	    </p>
		</td></tr>
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/aaai19_FHR.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1811.00342.pdf"><papertitle><b>Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos</b></papertitle></a><br>
		<b>Y. Tai*</b>, Y. Liang*, X. Liu, L. Duan, J. Li, C. Wang, F. Huang and Y. Chen.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
		<br>
		<a href="https://arxiv.org/pdf/1811.00342.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://drive.google.com/open?id=1bDxP_i2ETfoO4uAbPhukCcxvOrVQuwd-"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://drive.google.com/open?id=1cFyjZWdGOBZ8t-63bZehERMaKpTkawwe"; style="color: #EE7F2D;">Supp</a>
		/
		<a href="https://github.com/tyshiwo/FHR_alignment"; style="color: #EE7F2D;">Code</a>
		<br>
		<a class="github-button" href="https://github.com/TencentYoutuResearch/FaceAlignment-FHR" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/FHR_alignment on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/TencentYoutuResearch/FaceAlignment-FHR/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/FHR_alignment on GitHub">Fork</a>
	    </p>
		</td></tr>		
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DAML_aaai19.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"><papertitle><b>Data-Adaptive Metric Learning with Scale Alignment</b></papertitle></a><br>
		S. Chen, C. Gong, J. Yang, <b>Y. Tai</b>, L. Hui and J. Li.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
		<br>
		<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"; style="color: #EE7F2D;">Paper (Official)</a>
		<br>
	    </p>
		</td></tr>						


		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CD_ECCV18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"><papertitle><b>Person Search via A Mask-Guided Two-Stream CNN Model</b></papertitle></a><br>
		D. Chen, S. Zhang, W. Ouyang, J. Yang and <b>Y. Tai</b>.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2018
		<br>
		<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://drive.google.com/file/d/1Wg4ieK4BQc5XMZCBPpQJXumoHs21AHBM/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		<br>
	    </p>
		</td></tr>	
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <a href="./projects/faceSR.gif"><img src="./projects/faceSR.gif" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1711.10703.pdf"><papertitle><b>FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</b></papertitle></a><br>
		<b>Y. Tai*</b>, Y. Chen*, X. Liu, C. Shen, J. Yang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2018 <b style="color: red">[SPOTLIGHT]</b>
		<br>
		<a href="https://drive.google.com/file/d/10i2NZfUyf2Yold4ABusz3Que-XN_gEEu/view"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://arxiv.org/pdf/1711.10703.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://github.com/tyshiwo/FSRNet"; style="color: #EE7F2D;">Code</a>
		/
		<a href="https://www.youtube.com/watch?v=RgHum6TB5aM"; style="color: #EE7F2D;">Demo</a>
		/
		<a href="https://drive.google.com/open?id=1U117O2ZyBDKVpPL06JA3g3uFtL6vDsp0"; style="color: #EE7F2D;">Slides</a>
		/
		<a href="https://drive.google.com/file/d/1JaSUxohFUz8m9ukAFzCbiT281nSG5lkY/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		<br>
		<a class="github-button" href="https://github.com/tyshiwo/FSRNet" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/FSRNet on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/tyshiwo/FSRNet/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/FSRNet on GitHub">Fork</a>
	    </p>
		</td></tr>
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		 <img src="./projects/MemNet_iccv17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"><papertitle><b>MemNet: A Persistent Memory Network for Image Restoration</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu, C. Xu.
		<br>
        <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2017 <b style="color: red">[SPOTLIGHT]</b>
		<br>
		<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/tyshiwo/MemNet"; style="color: #EE7F2D;">Code</a>
		/
		<a href="https://drive.google.com/open?id=1maokCHq_EzOzft8UtaAFOOUzpgE6ehuk"; style="color: #EE7F2D;">Poster</a>
		<br>
		<a class="github-button" href="https://github.com/tyshiwo/MemNet" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/MemNet on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/tyshiwo/MemNet/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/MemNet on GitHub">Fork</a>
	    </p>
		</td></tr>					
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		 <img src="./projects/DRRN_cvpr17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"><papertitle><b>Image Super-Resolution via Deep Recursive Residual Network</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2017 
		<br>
		<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/tyshiwo/DRRN_CVPR17"; style="color: #EE7F2D;">Code</a>
		/
		<a href="http://cvlab.cse.msu.edu/project-super-resolution.html"; style="color: #EE7F2D;">Project</a>
		/
		<a href="https://drive.google.com/file/d/1rWtZL3rQK5uzlw5lyPabop7MrbE0KS7j/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		<br>
		<a class="github-button" href="https://github.com/tyshiwo/DRRN_CVPR17" data-icon="octicon-star" data-show-count="true" aria-label="Star tyshiwo/DRRN_CVPR17 on GitHub">Star</a>
		/
		<a class="github-button" href="https://github.com/tyshiwo/DRRN_CVPR17/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork tyshiwo/DRRN_CVPR17 on GitHub">Fork</a>
	    </p>
               <p><font color="red">My first paper that has over 1,000 google scholar citations</font></p> 
	    </p>
		</td></tr>	


        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/NMR_pami16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"><papertitle><b>Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes</b></papertitle></a><br>
		J. Yang, L. Luo, J. Qian, <b>Y. Tai</b>, F. Zhang and Y. Xu.
		<br>
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence</em>, 2017
		<br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>	
		
				
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SOPR_SDM16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"><papertitle><b>Structural Orthogonal Procrustes Regression for Face Recognition with Pose Variations and Misalignment</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, F. Zhang, Y. Zhang, L. Luo, J. Qian.
		<br>
        <em>SIAM Conference on Data Mining (<b>SDM</b>)</em>, 2016
		<br>
		<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>		
		
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/OPR_TIP16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"><papertitle><b>Face Recognition with Pose Variations and Misalignment via Orthogonal Procrustes Regression</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, Y. Zhang, L. Luo, J. Qian and Y. Chen
		<br>
        <em>IEEE Trans. on Image Processing</em>, 2016
		<br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>		
		
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/LDSVDR_PR2016.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"><papertitle><b>Learning Discriminative Singular Value Decomposition Representation for Face Recognition</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, L. Luo, F. Zhang and J. Qian
		<br>
        <em>Pattern Recognition</em>, 2016
		<br>
		<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>				
						       
</table></p>
		
	<p style="text-align:left"><strong><font size="4px">Awards</font></strong></p>
		<ul>
		    <li><p style="text-align:left">2021 Winner of CVPR NTIRE 2021 Challenge on Video Super-Resolution: Spatial-Temporal (Team name: Imagination)</p></li>
		    <li><p style="text-align:left">2020 Winner of CVPR NTIRE 2020 Challenge on Real-World Super-Resolution (Team name: Impressionism)</p></li>
			<li><p style="text-align:left">2018 Stars of Youtu Lab, Tencent </p></li>
			<li><p style="text-align:left">2018, 2019, 2020 Outstanding Staff Award, Tencent </p></li>
			<li><p style="text-align:left">2018 Excellent Doctoral Dissertation of Nanjing University of Science and Technology, China </p></li>
			<li><p style="text-align:left">ICCV'17 Student Volunteer Travel Award </p></li>
			<li><p style="text-align:left">2017 Outstanding Graduate </p></li>
			<li><p style="text-align:left">2016 National Graduate Scholarship </p></li>
		</ul><br>

	<p style="text-align:justify"><strong><font size="4px">Professional activities</font></strong></p>
		<ul>
			<li><p style="text-align:left">Reviewer for CVPR'(17, 18, 19, 20, 21), ICCV'(17,19), ECCV'(18, 20), AAAI'(19, 20), ICLR'20, NIPS'20 </p></li>
			<li>
			  <p style="text-align:left">Reviewer for Trans. on Pattern Analysis and Machine Intelligence (TPAMI), International Journal of Computer Vision (IJCV), IEEE Trans. on Image Processing (TIP), Pattern Recognition, Pattern Recognition Letters </p></li>
	    </ur>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=faf9f9&w=300&t=tt&d=hvoHWGKZcyFl6zhd6aLhpusD9f4jQY_gzPG8UfsmW0I&co=1285d6'></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
              Last modified in Apr. 2021.
              For the style of my personal website, Please refer to the wonderful page from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              <br>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>          

</html>
